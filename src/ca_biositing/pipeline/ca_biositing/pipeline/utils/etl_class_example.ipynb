{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ad85317",
      "metadata": {},
      "source": [
        "# ETL Class Example Notebook\n",
        "\n",
        "This notebook demonstrates the three main steps of the ETL pipeline: **Extract**, **Transform**, and **Load**. It mirrors the structure of `etl_notebook.ipynb` but provides a concise classâ€‘based example for quick reference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19476297",
      "metadata": {},
      "source": [
        "## Extract\n",
        "\n",
        "Set up the project root on `sys.path` so that package imports work from any working directory. Import the extraction utilities required for this example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "eb3498d9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:00.900 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Extracting raw data from '03.1-Proximate' in 'Aim 1-Feedstock Collection and Processing Data-BioCirV'...\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:00.900 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Extracting raw data from '03.1-Proximate' in 'Aim 1-Feedstock Collection and Processing Data-BioCirV'...\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:03.401 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Successfully extracted raw data.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:03.401 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Successfully extracted raw data.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:03.403 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:03.403 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:03.423 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Extracting raw data from '03.7-Ultimate' in 'Aim 1-Feedstock Collection and Processing Data-BioCirV'...\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:03.423 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Extracting raw data from '03.7-Ultimate' in 'Aim 1-Feedstock Collection and Processing Data-BioCirV'...\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:04.904 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Successfully extracted raw data.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:04.904 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Successfully extracted raw data.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:04.906 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:04.906 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:04.926 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Extracting raw data from '03.3-CmpAna' in 'Aim 1-Feedstock Collection and Processing Data-BioCirV'...\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:04.926 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Extracting raw data from '03.3-CmpAna' in 'Aim 1-Feedstock Collection and Processing Data-BioCirV'...\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:06.452 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Successfully extracted raw data.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:06.452 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Successfully extracted raw data.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:06.455 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:06.455 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 3 dataframes.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.dialects.postgresql import insert\n",
        "from sqlalchemy.orm import Session\n",
        "\n",
        "# Find the project root (directory containing 'pixi.toml') \n",
        "path = os.getcwd()\n",
        "project_root = None\n",
        "while path != os.path.dirname(path):\n",
        "    if 'pixi.toml' in os.listdir(path):\n",
        "        project_root = path\n",
        "        break\n",
        "    path = os.path.dirname(path)\n",
        "\n",
        "if project_root is None:\n",
        "    raise FileNotFoundError('Could not locate project root')\n",
        "\n",
        "# Ensure the root is on the Python path\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from ca_biositing.datamodels.config import settings\n",
        "from ca_biositing.pipeline.etl.extract import proximate, ultimate, cmpana\n",
        "\n",
        "# Extract data\n",
        "prox_raw = proximate.extract()\n",
        "ult_raw = ultimate.extract()\n",
        "cmpana_raw = cmpana.extract()\n",
        "\n",
        "analysis_data = [prox_raw, ult_raw, cmpana_raw]\n",
        "print(f\"Extracted {len(analysis_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476199bd",
      "metadata": {},
      "source": [
        "## Transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d2781377",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned 3 dataframes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/pjsmitty301/ca-biositing/src/ca_biositing/pipeline/ca_biositing/pipeline/utils/cleaning_functions/cleaning.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  return df.astype(\"object\").replace(regex, np.nan, regex=True)\n",
            "/Users/pjsmitty301/ca-biositing/src/ca_biositing/pipeline/ca_biositing/pipeline/utils/cleaning_functions/cleaning.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  return df.astype(\"object\").replace(regex, np.nan, regex=True)\n",
            "/Users/pjsmitty301/ca-biositing/src/ca_biositing/pipeline/ca_biositing/pipeline/utils/cleaning_functions/cleaning.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  return df.astype(\"object\").replace(regex, np.nan, regex=True)\n"
          ]
        }
      ],
      "source": [
        "from ca_biositing.pipeline.utils.cleaning_functions import cleaning as cleaning_mod\n",
        "\n",
        "cleaned_data = []\n",
        "for df in analysis_data:\n",
        "    df['dataset'] = 'biocirv'\n",
        "    cleaned_df = cleaning_mod.standard_clean(df)\n",
        "    cleaned_data.append(cleaned_df)\n",
        "\n",
        "print(f\"Cleaned {len(cleaned_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "190ad4ca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coerced 3 dataframes.\n"
          ]
        }
      ],
      "source": [
        "from ca_biositing.pipeline.utils.cleaning_functions import coercion as coercion_mod\n",
        "\n",
        "coerced_data = []\n",
        "for df in cleaned_data:\n",
        "    # Example: coerce columns into the designated data types (int, float, datetime, geom, etc)\n",
        "    coerced_df = coercion_mod.coerce_columns(df,\n",
        "                                             int_cols=['repl_no'], \n",
        "                                             float_cols=['value'],\n",
        "                                             datetime_cols=['created_at', 'updated_at'])\n",
        "    coerced_data.append(coerced_df)\n",
        "\n",
        "print(f\"Coerced {len(coerced_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "032f93cd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:45.573 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | ca_biositing.pipeline.utils.name_id_swap - Column 'sample_unit' missing in DataFrame #1; skipping.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:45.573 | \u001b[38;5;184mWARNING\u001b[0m | ca_biositing.pipeline.utils.name_id_swap - Column 'sample_unit' missing in DataFrame #1; skipping.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:45.575 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | ca_biositing.pipeline.utils.name_id_swap - Column 'primary_ag_product' missing in DataFrame #1; skipping.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:45.575 | \u001b[38;5;184mWARNING\u001b[0m | ca_biositing.pipeline.utils.name_id_swap - Column 'primary_ag_product' missing in DataFrame #1; skipping.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:45.576 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | ca_biositing.pipeline.utils.name_id_swap - Column 'provider_code' missing in DataFrame #1; skipping.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:45.576 | \u001b[38;5;184mWARNING\u001b[0m | ca_biositing.pipeline.utils.name_id_swap - Column 'provider_code' missing in DataFrame #1; skipping.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:45.585 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | ca_biositing.pipeline.utils.name_id_swap - Column 'sample_unit' missing in DataFrame #1; skipping.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:45.585 | \u001b[38;5;184mWARNING\u001b[0m | ca_biositing.pipeline.utils.name_id_swap - Column 'sample_unit' missing in DataFrame #1; skipping.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:45.585 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | ca_biositing.pipeline.utils.name_id_swap - Column 'primary_ag_product' missing in DataFrame #1; skipping.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:45.585 | \u001b[38;5;184mWARNING\u001b[0m | ca_biositing.pipeline.utils.name_id_swap - Column 'primary_ag_product' missing in DataFrame #1; skipping.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:45.586 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | ca_biositing.pipeline.utils.name_id_swap - Column 'provider_code' missing in DataFrame #1; skipping.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:45.586 | \u001b[38;5;184mWARNING\u001b[0m | ca_biositing.pipeline.utils.name_id_swap - Column 'provider_code' missing in DataFrame #1; skipping.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:45.603 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | ca_biositing.pipeline.utils.name_id_swap - Column 'sample_unit' missing in DataFrame #1; skipping.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:45.603 | \u001b[38;5;184mWARNING\u001b[0m | ca_biositing.pipeline.utils.name_id_swap - Column 'sample_unit' missing in DataFrame #1; skipping.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:45.607 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | ca_biositing.pipeline.utils.name_id_swap - Column 'primary_ag_product' missing in DataFrame #1; skipping.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:45.607 | \u001b[38;5;184mWARNING\u001b[0m | ca_biositing.pipeline.utils.name_id_swap - Column 'primary_ag_product' missing in DataFrame #1; skipping.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">10:14:45.607 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | ca_biositing.pipeline.utils.name_id_swap - Column 'provider_code' missing in DataFrame #1; skipping.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "10:14:45.607 | \u001b[38;5;184mWARNING\u001b[0m | ca_biositing.pipeline.utils.name_id_swap - Column 'provider_code' missing in DataFrame #1; skipping.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized 3 dataframes.\n"
          ]
        }
      ],
      "source": [
        "from ca_biositing.pipeline.utils.name_id_swap import normalize_dataframes\n",
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import *\n",
        "\n",
        "normalize_columns = {\n",
        "    'resource': (Resource, 'name'),\n",
        "    'prepared_sample': (PreparedSample, 'name'),\n",
        "    'preparation_method': (Method, 'name'),\n",
        "    'parameter': (Parameter, 'name'),\n",
        "    'unit': (Unit, 'name'),\n",
        "    'sample_unit': (Unit, 'name'),\n",
        "    'analyst_email': (Contact, 'email'),\n",
        "    'primary_ag_product': (PrimaryAgProduct, 'name'),\n",
        "    'provider_code': (Provider, 'codename'),\n",
        "    'dataset': (Dataset, 'name')\n",
        "}\n",
        "\n",
        "normalized_data = []\n",
        "for df in coerced_data:\n",
        "    normalized_df = normalize_dataframes(df, normalize_columns)\n",
        "    normalized_data.append(normalized_df)\n",
        "\n",
        "print(f\"Normalized {len(normalized_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c2def234",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 3 observation dataframes.\n"
          ]
        }
      ],
      "source": [
        "## Prepare Record Information DataFrames\n",
        "\n",
        "observation_data = []\n",
        "for df in normalized_data:\n",
        "    obs_df = df[[\n",
        "        'dataset_id',\n",
        "        'analysis_type', \n",
        "        'record_id',\n",
        "        'parameter_id',\n",
        "        'value',\n",
        "        'unit_id', \n",
        "        'note'\n",
        "    ]].copy().rename(columns={'analysis_type': 'record_type'})\n",
        "    obs_df = obs_df.dropna(subset=['record_id', 'parameter_id', 'value'])\n",
        "    observation_data.append(obs_df)\n",
        "\n",
        "print(f\"Prepared {len(observation_data)} observation dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "145e4834",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 3 record dataframes.\n"
          ]
        }
      ],
      "source": [
        "## Prepare Record Information DataFrames\n",
        "\n",
        "record_data = []\n",
        "for df in normalized_data:\n",
        "    # 1. Define explicit mappings for non-normalized columns\n",
        "    rename_map = {\n",
        "        'record_id': 'record_id',\n",
        "        'repl_no': 'technical_replication_no',\n",
        "        'qc_result': 'qc_pass',\n",
        "        'note': 'note'\n",
        "    }\n",
        "    \n",
        "    # 2. Dynamically add normalized columns from the normalize_columns dictionary\n",
        "    for col in normalize_columns.keys():\n",
        "        norm_col = f\"{col}_id\"\n",
        "        if norm_col in df.columns:\n",
        "            # Special case: rename to match target record table schema\n",
        "            target_name = 'analyst_id' if col == 'analyst_email' else \\\n",
        "                          'method_id' if col == 'preparation_method' else norm_col\n",
        "            rename_map[norm_col] = target_name\n",
        "\n",
        "    # 3. Only select columns that actually exist in this specific dataframe\n",
        "    available_cols = [c for c in rename_map.keys() if c in df.columns]\n",
        "    final_rename = {k: v for k, v in rename_map.items() if k in available_cols}\n",
        "\n",
        "    record_df = df[available_cols].copy().rename(columns=final_rename)\n",
        "\n",
        "    # 4. Drop rows where critical identifiers are missing (NaN)\n",
        "    if 'record_id' in record_df.columns:\n",
        "        record_df = record_df.dropna(subset=['record_id'])\n",
        "    \n",
        "    record_data.append(record_df)\n",
        "\n",
        "print(f\"Prepared {len(record_data)} record dataframes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d92bf7",
      "metadata": {},
      "source": [
        "## Load\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "10cf570b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upsert of all observations completed.\n"
          ]
        }
      ],
      "source": [
        "db_url = settings.database_url\n",
        "if \"@db:\" in db_url:\n",
        "    db_url = db_url.replace(\"@db:\", \"@localhost:\")\n",
        "elif \"db:5432\" in db_url:\n",
        "    db_url = db_url.replace(\"db:5432\", \"localhost:5432\")\n",
        "\n",
        "engine = create_engine(db_url)\n",
        "\n",
        "def upsert_observations(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        record['updated_at'] = now\n",
        "        if record.get('created_at') is None:\n",
        "            record['created_at'] = now\n",
        "        stmt = insert(Observation).values(record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in Observation.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    for obs_df in observation_data:\n",
        "        upsert_observations(obs_df, session)\n",
        "    session.commit()\n",
        "print('Upsert of all observations completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "upsert-proximate",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upsert of Proximate records completed.\n"
          ]
        }
      ],
      "source": [
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import ProximateRecord\n",
        "\n",
        "def upsert_proximate_records(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    # Filter record dictionary to only include columns that exist in the table\n",
        "    table_columns = {c.name for c in ProximateRecord.__table__.columns}\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
        "        clean_record['updated_at'] = now\n",
        "        if clean_record.get('created_at') is None:\n",
        "            clean_record['created_at'] = now\n",
        "        stmt = insert(ProximateRecord).values(clean_record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in ProximateRecord.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    # Assuming the first dataframe in record_data is Proximate\n",
        "    upsert_proximate_records(record_data[0], session)\n",
        "    session.commit()\n",
        "print('Upsert of Proximate records completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "upsert-ultimate",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upsert of Ultimate records completed.\n"
          ]
        }
      ],
      "source": [
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import UltimateRecord\n",
        "\n",
        "def upsert_ultimate_records(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    table_columns = {c.name for c in UltimateRecord.__table__.columns}\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
        "        clean_record['updated_at'] = now\n",
        "        if clean_record.get('created_at') is None:\n",
        "            clean_record['created_at'] = now\n",
        "        stmt = insert(UltimateRecord).values(clean_record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in UltimateRecord.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    # Assuming the second dataframe in record_data is Ultimate\n",
        "    upsert_ultimate_records(record_data[1], session)\n",
        "    session.commit()\n",
        "print('Upsert of Ultimate records completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "upsert-compositional",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upsert of Compositional records completed.\n"
          ]
        }
      ],
      "source": [
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import CompositionalRecord\n",
        "\n",
        "def upsert_compositional_records(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    table_columns = {c.name for c in CompositionalRecord.__table__.columns}\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
        "        clean_record['updated_at'] = now\n",
        "        if clean_record.get('created_at') is None:\n",
        "            clean_record['created_at'] = now\n",
        "        stmt = insert(CompositionalRecord).values(clean_record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in CompositionalRecord.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    # Assuming the third dataframe in record_data is Compositional\n",
        "    upsert_compositional_records(record_data[2], session)\n",
        "    session.commit()\n",
        "print('Upsert of Compositional records completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2833a54e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[     record_id  technical_replication_no qc_pass  note  resource_id  \\\n",
              " 0     (73)b7b7                         1    pass  <NA>          5.0   \n",
              " 1     (73)9ba8                         2    pass  <NA>          5.0   \n",
              " 2     (73)957d                         3    pass  <NA>          5.0   \n",
              " 3     (73)9335                         1    pass  <NA>          5.0   \n",
              " 4     (73)415f                         2    pass  <NA>          5.0   \n",
              " ...        ...                       ...     ...   ...          ...   \n",
              " 1027  (f3)c096                         2    pass  <NA>          6.0   \n",
              " 1028  (f3)4605                         3    pass  <NA>          6.0   \n",
              " 1029  (b5)c2b4                         1    pass  <NA>         43.0   \n",
              " 1030  (b5)889c                         2    pass  <NA>         43.0   \n",
              " 1031  (b5)132e                         3    pass  <NA>         43.0   \n",
              " \n",
              "       prepared_sample_id  method_id  parameter_id  unit_id  analyst_id  \\\n",
              " 0                   41.0        5.0           4.0        1         1.0   \n",
              " 1                   41.0        5.0           4.0        1         1.0   \n",
              " 2                   41.0        5.0           4.0        1         1.0   \n",
              " 3                   41.0        5.0           3.0        1         1.0   \n",
              " 4                   41.0        5.0           3.0        1         1.0   \n",
              " ...                  ...        ...           ...      ...         ...   \n",
              " 1027               114.0        5.0           1.0        1         NaN   \n",
              " 1028               114.0        5.0           1.0        1         NaN   \n",
              " 1029               115.0        5.0           1.0        1         NaN   \n",
              " 1030               115.0        5.0           1.0        1         NaN   \n",
              " 1031               115.0        5.0           1.0        1         NaN   \n",
              " \n",
              "       dataset_id  \n",
              " 0              1  \n",
              " 1              1  \n",
              " 2              1  \n",
              " 3              1  \n",
              " 4              1  \n",
              " ...          ...  \n",
              " 1027           1  \n",
              " 1028           1  \n",
              " 1029           1  \n",
              " 1030           1  \n",
              " 1031           1  \n",
              " \n",
              " [1032 rows x 11 columns],\n",
              "    record_id  technical_replication_no qc_pass     note  resource_id  \\\n",
              " 0   )u22e822                         1    pass        1           25   \n",
              " 1   )u001200                         1    fail    1 dup           25   \n",
              " 2   )ud275d2                         1    pass        2            9   \n",
              " 3   )ud354d3                         1    pass        3           10   \n",
              " 4   )u670267                         1    pass        4           19   \n",
              " ..       ...                       ...     ...      ...          ...   \n",
              " 59  )u91a791                         1    pass   10 dup            5   \n",
              " 60  )uf825f8                         1    pass       11           33   \n",
              " 61  7ud442d4                         1    pass       12           22   \n",
              " 62  )u608260                         1    pass       13           19   \n",
              " 63  )u87a587                         1    pass   13 dup           19   \n",
              " \n",
              "     prepared_sample_id  method_id  parameter_id  unit_id  dataset_id  \n",
              " 0                   10          3             5        2           1  \n",
              " 1                   10          3             5        2           1  \n",
              " 2                    7          3             5        2           1  \n",
              " 3                   85          2             5        2           1  \n",
              " 4                   94          2             5        2           1  \n",
              " ..                 ...        ...           ...      ...         ...  \n",
              " 59                  93          2             6        2           1  \n",
              " 60                  21          3             6        2           1  \n",
              " 61                  88          4             6        2           1  \n",
              " 62                  91          2             6        2           1  \n",
              " 63                  91          2             6        2           1  \n",
              " \n",
              " [64 rows x 10 columns],\n",
              "     record_id  technical_replication_no qc_pass  note  resource_id  \\\n",
              " 0    (85)e114                         1    pass  <NA>          5.0   \n",
              " 1    (85)41cb                         2    pass  <NA>          5.0   \n",
              " 2    (85)2246                         3    pass  <NA>          5.0   \n",
              " 3    (85)9311                         1    pass  <NA>          5.0   \n",
              " 4    (85)63d7                         2    pass  <NA>          5.0   \n",
              " ..        ...                       ...     ...   ...          ...   \n",
              " 652  (d4)96b0                         2    <NA>  <NA>         14.0   \n",
              " 653  (d4)b237                         3    <NA>  <NA>         14.0   \n",
              " 654  (f2)ac80                         1    <NA>  <NA>         28.0   \n",
              " 655  (f2)01cf                         2    <NA>  <NA>         28.0   \n",
              " 656  (f2)89cd                         3    <NA>  <NA>         28.0   \n",
              " \n",
              "      prepared_sample_id  method_id  parameter_id  unit_id  analyst_id  \\\n",
              " 0                  97.0        1.0          16.0        3         1.0   \n",
              " 1                  97.0        1.0          16.0        3         1.0   \n",
              " 2                  97.0        1.0          16.0        3         1.0   \n",
              " 3                  97.0        1.0          14.0        3         1.0   \n",
              " 4                  97.0        1.0          14.0        3         1.0   \n",
              " ..                  ...        ...           ...      ...         ...   \n",
              " 652                98.0        2.0          15.0        3         NaN   \n",
              " 653                98.0        2.0          15.0        3         NaN   \n",
              " 654               100.0        2.0          15.0        3         NaN   \n",
              " 655               100.0        2.0          15.0        3         NaN   \n",
              " 656               100.0        2.0          15.0        3         NaN   \n",
              " \n",
              "      dataset_id  \n",
              " 0             1  \n",
              " 1             1  \n",
              " 2             1  \n",
              " 3             1  \n",
              " 4             1  \n",
              " ..          ...  \n",
              " 652           1  \n",
              " 653           1  \n",
              " 654           1  \n",
              " 655           1  \n",
              " 656           1  \n",
              " \n",
              " [657 rows x 11 columns]]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "record_data"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ca-biositing (Pixi)",
      "language": "python",
      "name": "ca-biositing-pixi"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
