{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ad85317",
      "metadata": {},
      "source": [
        "# ETL Class Example Notebook\n",
        "\n",
        "This notebook demonstrates the three main steps of the ETL pipeline: **Extract**, **Transform**, and **Load**. It mirrors the structure of `etl_notebook.ipynb` but provides a concise class‑based example for quick reference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19476297",
      "metadata": {},
      "source": [
        "## Extract\n",
        "\n",
        "Set up the project root on `sys.path` so that package imports work from any working directory. Import the extraction utilities required for this example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3498d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "# Find the project root (directory containing 'pixi.toml')\n",
        "path = os.getcwd()\n",
        "project_root = None\n",
        "while path != os.path.dirname(path):\n",
        "    if 'pixi.toml' in os.listdir(path):\n",
        "        project_root = path\n",
        "        break\n",
        "    path = os.path.dirname(path)\n",
        "\n",
        "if project_root is None:\n",
        "    raise FileNotFoundError('Could not locate project root')\n",
        "# Ensure the root is on the Python path\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Example extraction – import a sample extractor (adjust as needed)\n",
        "from src.ca_biositing.pipeline.ca_biositing.pipeline.etl.extract import proximate\n",
        "raw_df = proximate.extract(project_root=project_root)\n",
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476199bd",
      "metadata": {},
      "source": [
        "## Transform\n",
        "\n",
        "Apply cleaning, coercion, and normalization utilities from the `cleaning_functions` package. Each utility is demonstrated in its own cell for clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2781377",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleaning utilities\n",
        "from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.cleaning_functions import cleaning as cleaning_mod\n",
        "# Apply the standard cleaning pipeline to the extracted dataframe\n",
        "cleaned_df = cleaning_mod.standard_clean(raw_df)\n",
        "cleaned_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "190ad4ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coercion utilities\n",
        "from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.cleaning_functions import coercion as coercion_mod\n",
        "# Example: coerce integer and float columns\n",
        "coerced_df = coercion_mod.coerce_columns(cleaned_df, \n",
        "                                         int_cols=['repl_no'], \n",
        "                                         float_cols=['value'],\n",
        "                                         datetime_cols=['created_at', 'updated_at'])\n",
        "coerced_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032f93cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalization (name → id) utilities\n",
        "from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.name_id_swap import replace_name_with_id_df\n",
        "\n",
        "# Import reference models\n",
        "from src.ca_biositing.datamodels.ca_biositing.datamodels.schemas.generated.ca_biositing import *\n",
        "# Define columns to normalize – map column name to (Model, attribute)\n",
        "NORMALIZE_MAP = {\n",
        "    'resource': (Resource, 'name'),\n",
        "    'prepared_sample': (PreparedSample, 'name'),\n",
        "    'preparation_method': (PreparationMethod, 'name'),\n",
        "    'parameter': (Parameter, 'name'),\n",
        "    'unit': (Unit, 'name'),\n",
        "    'sample_unit': (Unit, 'name'),\n",
        "    'analyst_email': (Contact, 'email'),\n",
        "    'analysis_type': (AnalysisType, 'name'),\n",
        "    'primary_ag_product': (PrimaryAgProduct, 'name'),\n",
        "    'provider_code': (Provider, 'codename')\n",
        "    # Add additional mappings as required\n",
        "}\n",
        "norm_df = coerced_df.copy()\n",
        "for col, (model, attr) in NORMALIZE_MAP.items():\n",
        "    if col in norm_df.columns:\n",
        "        norm_df, _ = replace_name_with_id_df(db=None, df=norm_df, ref_model=model, df_name_column=col, model_name_attr=attr, id_column_name='id', final_column_name=f'{col}_id')\n",
        "norm_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d92bf7",
      "metadata": {},
      "source": [
        "## Load\n",
        "\n",
        "Load the transformed dataframes into the database. The example below re‑uses the load logic from `etl/load/products/primary_ag_product.py` and extends it to handle a list of tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cf570b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.ca_biositing.pipeline.ca_biositing.pipeline.etl.load.products.primary_ag_product import load_primary_ag_product\n",
        "from sqlmodel import Session\n",
        "from src.ca_biositing.datamodels.ca_biositing.datamodels.database import get_engine\n",
        "\n",
        "engine = get_engine()\n",
        "# Example: list of (dataframe, load_function) pairs\n",
        "load_tasks = [\n",
        "    (norm_df, load_primary_ag_product),\n",
        "    # Add additional (df, load_fn) tuples for other tables\n",
        "]\n",
        "with Session(engine) as session:\n",
        "    for df, load_fn in load_tasks:\n",
        "        if df is not None and not df.empty:\n",
        "            load_fn(df=df, db=session)\n",
        "    session.commit()\n",
        "print('Load step completed')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "default",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
