{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ad85317",
      "metadata": {},
      "source": [
        "# ETL Class Example Notebook\n",
        "\n",
        "This notebook demonstrates the three main steps of the ETL pipeline: **Extract**, **Transform**, and **Load**. It mirrors the structure of `etl_notebook.ipynb` but provides a concise classâ€‘based example for quick reference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19476297",
      "metadata": {},
      "source": [
        "## Extract\n",
        "\n",
        "Set up the project root on `sys.path` so that package imports work from any working directory. Import the extraction utilities required for this example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3498d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.dialects.postgresql import insert\n",
        "from sqlalchemy.orm import Session\n",
        "\n",
        "# Find the project root (directory containing 'pixi.toml') \n",
        "path = os.getcwd()\n",
        "project_root = None\n",
        "while path != os.path.dirname(path):\n",
        "    if 'pixi.toml' in os.listdir(path):\n",
        "        project_root = path\n",
        "        break\n",
        "    path = os.path.dirname(path)\n",
        "\n",
        "if project_root is None:\n",
        "    raise FileNotFoundError('Could not locate project root')\n",
        "\n",
        "# Ensure the root is on the Python path\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from ca_biositing.datamodels.config import settings\n",
        "from ca_biositing.pipeline.etl.extract import proximate, ultimate, cmpana\n",
        "\n",
        "# Extract data\n",
        "prox_raw = proximate.extract()\n",
        "ult_raw = ultimate.extract()\n",
        "cmpana_raw = cmpana.extract()\n",
        "\n",
        "analysis_data = [prox_raw, ult_raw, cmpana_raw]\n",
        "print(f\"Extracted {len(analysis_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476199bd",
      "metadata": {},
      "source": [
        "## Transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2781377",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.pipeline.utils.cleaning_functions import cleaning as cleaning_mod\n",
        "\n",
        "cleaned_data = []\n",
        "for df in analysis_data:\n",
        "    df['dataset'] = 'biocirv'\n",
        "    cleaned_df = cleaning_mod.standard_clean(df)\n",
        "    cleaned_data.append(cleaned_df)\n",
        "\n",
        "print(f\"Cleaned {len(cleaned_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "190ad4ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.pipeline.utils.cleaning_functions import coercion as coercion_mod\n",
        "\n",
        "coerced_data = []\n",
        "for df in cleaned_data:\n",
        "    # Example: coerce columns into the designated data types (int, float, datetime, geom, etc)\n",
        "    coerced_df = coercion_mod.coerce_columns(df,\n",
        "                                             int_cols=['repl_no'], \n",
        "                                             float_cols=['value'],\n",
        "                                             datetime_cols=['created_at', 'updated_at'])\n",
        "    coerced_data.append(coerced_df)\n",
        "\n",
        "print(f\"Coerced {len(coerced_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032f93cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.pipeline.utils.name_id_swap import normalize_dataframes\n",
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import *\n",
        "\n",
        "normalize_columns = {\n",
        "    'resource': (Resource, 'name'),\n",
        "    'prepared_sample': (PreparedSample, 'name'),\n",
        "    'preparation_method': (PreparationMethod, 'name'),\n",
        "    'parameter': (Parameter, 'name'),\n",
        "    'unit': (Unit, 'name'),\n",
        "    'sample_unit': (Unit, 'name'),\n",
        "    'analyst_email': (Contact, 'email'),\n",
        "    'primary_ag_product': (PrimaryAgProduct, 'name'),\n",
        "    'provider_code': (Provider, 'codename'),\n",
        "    'dataset': (Dataset, 'name')\n",
        "}\n",
        "\n",
        "normalized_data = []\n",
        "for df in coerced_data:\n",
        "    normalized_df = normalize_dataframes(df, normalize_columns)\n",
        "    normalized_data.append(normalized_df)\n",
        "\n",
        "print(f\"Normalized {len(normalized_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2def234",
      "metadata": {},
      "outputs": [],
      "source": [
        "observation_data = []\n",
        "for df in normalized_data:\n",
        "    obs_df = df[[\n",
        "        'dataset_id',\n",
        "        'analysis_type', \n",
        "        'record_id',\n",
        "        'parameter_id',\n",
        "        'value',\n",
        "        'unit_id', \n",
        "        'note'\n",
        "    ]].copy().rename(columns={'analysis_type': 'record_type'})\n",
        "    obs_df = obs_df.dropna(subset=['record_id', 'parameter_id', 'value'])\n",
        "    observation_data.append(obs_df)\n",
        "\n",
        "print(f\"Prepared {len(observation_data)} observation dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145e4834",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Prepare Record Information DataFrames\n",
        "\n",
        "record_data = []\n",
        "for df in normalized_data:\n",
        "    # 1. Define explicit mappings for non-normalized columns\n",
        "    rename_map = {\n",
        "        'record_id': 'record_id',\n",
        "        'repl_no': 'technical_replication_no',\n",
        "        'qc_result': 'qc_pass',\n",
        "        'note': 'note'\n",
        "    }\n",
        "    \n",
        "    # 2. Dynamically add normalized columns from the normalize_columns dictionary\n",
        "    for col in normalize_columns.keys():\n",
        "        norm_col = f\"{col}_id\"\n",
        "        if norm_col in df.columns:\n",
        "            # Special case: rename to match target record table schema\n",
        "            target_name = 'analyst_id' if col == 'analyst_email' else \\\n",
        "                          'method_id' if col == 'preparation_method' else norm_col\n",
        "            rename_map[norm_col] = target_name\n",
        "\n",
        "    # 3. Only select columns that actually exist in this specific dataframe\n",
        "    available_cols = [c for c in rename_map.keys() if c in df.columns]\n",
        "    final_rename = {k: v for k, v in rename_map.items() if k in available_cols}\n",
        "\n",
        "    record_df = df[available_cols].copy().rename(columns=final_rename)\n",
        "\n",
        "    # 4. Drop rows where critical identifiers are missing (NaN)\n",
        "    if 'record_id' in record_df.columns:\n",
        "        record_df = record_df.dropna(subset=['record_id'])\n",
        "    \n",
        "    record_data.append(record_df)\n",
        "\n",
        "print(f\"Prepared {len(record_data)} record dataframes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d92bf7",
      "metadata": {},
      "source": [
        "## Load\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cf570b",
      "metadata": {},
      "outputs": [],
      "source": [
        "db_url = settings.database_url\n",
        "if \"@db:\" in db_url:\n",
        "    db_url = db_url.replace(\"@db:\", \"@localhost:\")\n",
        "elif \"db:5432\" in db_url:\n",
        "    db_url = db_url.replace(\"db:5432\", \"localhost:5432\")\n",
        "\n",
        "engine = create_engine(db_url)\n",
        "\n",
        "def upsert_observations(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        record['updated_at'] = now\n",
        "        if record.get('created_at') is None:\n",
        "            record['created_at'] = now\n",
        "        stmt = insert(Observation).values(record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in Observation.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    for obs_df in observation_data:\n",
        "        upsert_observations(obs_df, session)\n",
        "    session.commit()\n",
        "print('Upsert of all observations completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upsert-proximate",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import ProximateRecord\n",
        "\n",
        "def upsert_proximate_records(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    # Filter record dictionary to only include columns that exist in the table\n",
        "    table_columns = {c.name for c in ProximateRecord.__table__.columns}\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
        "        clean_record['updated_at'] = now\n",
        "        if clean_record.get('created_at') is None:\n",
        "            clean_record['created_at'] = now\n",
        "        stmt = insert(ProximateRecord).values(clean_record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in ProximateRecord.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    # Assuming the first dataframe in record_data is Proximate\n",
        "    upsert_proximate_records(record_data[0], session)\n",
        "    session.commit()\n",
        "print('Upsert of Proximate records completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upsert-ultimate",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import UltimateRecord\n",
        "\n",
        "def upsert_ultimate_records(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    table_columns = {c.name for c in UltimateRecord.__table__.columns}\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
        "        clean_record['updated_at'] = now\n",
        "        if clean_record.get('created_at') is None:\n",
        "            clean_record['created_at'] = now\n",
        "        stmt = insert(UltimateRecord).values(clean_record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in UltimateRecord.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    # Assuming the second dataframe in record_data is Ultimate\n",
        "    upsert_ultimate_records(record_data[1], session)\n",
        "    session.commit()\n",
        "print('Upsert of Ultimate records completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upsert-compositional",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import CompositionalRecord\n",
        "\n",
        "def upsert_compositional_records(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    table_columns = {c.name for c in CompositionalRecord.__table__.columns}\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
        "        clean_record['updated_at'] = now\n",
        "        if clean_record.get('created_at') is None:\n",
        "            clean_record['created_at'] = now\n",
        "        stmt = insert(CompositionalRecord).values(clean_record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in CompositionalRecord.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    # Assuming the third dataframe in record_data is Compositional\n",
        "    upsert_compositional_records(record_data[2], session)\n",
        "    session.commit()\n",
        "print('Upsert of Compositional records completed.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ca-biositing (Pixi)",
      "language": "python",
      "name": "ca-biositing-pixi"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
