{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8ad85317",
      "metadata": {},
      "source": [
        "# ETL Class Example Notebook\n",
        "\n",
        "This notebook demonstrates the three main steps of the ETL pipeline: **Extract**, **Transform**, and **Load**. It mirrors the structure of `etl_notebook.ipynb` but provides a concise classâ€‘based example for quick reference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19476297",
      "metadata": {},
      "source": [
        "## Extract\n",
        "\n",
        "Set up the project root on `sys.path` so that package imports work from any working directory. Import the extraction utilities required for this example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3498d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.dialects.postgresql import insert\n",
        "from sqlalchemy.orm import Session\n",
        "\n",
        "\n",
        "# Find the project root (directory containing 'pixi.toml') \n",
        "path = os.getcwd()\n",
        "project_root = None\n",
        "while path != os.path.dirname(path):\n",
        "    if 'pixi.toml' in os.listdir(path):\n",
        "        project_root = path\n",
        "        break\n",
        "    path = os.path.dirname(path)\n",
        "\n",
        "if project_root is None:\n",
        "    raise FileNotFoundError('Could not locate project root')\n",
        "\n",
        "# Ensure the root is on the Python path\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18423174",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from ca_biositing.datamodels.config import settings\n",
        "from ca_biositing.pipeline.etl.extract import samplemetadata, provider_info\n",
        "\n",
        "# Extract data\n",
        "samplemetadata = samplemetadata.extract()\n",
        "provider_info = provider_info.extract()\n",
        "\n",
        "sampling_data = [samplemetadata, provider_info]\n",
        "\n",
        "samplemetadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476199bd",
      "metadata": {},
      "source": [
        "## Transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2781377",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.pipeline.utils.cleaning_functions import cleaning as cleaning_mod\n",
        "\n",
        "cleaned_data = []\n",
        "for df in sampling_data:\n",
        "    df['dataset'] = 'biocirv'\n",
        "    cleaned_df = cleaning_mod.standard_clean(df)\n",
        "    cleaned_data.append(cleaned_df)\n",
        "\n",
        "print(f\"Cleaned {len(cleaned_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d17080b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "    if etl_run_id:\n",
        "        cleaned_df['etl_run_id'] = etl_run_id\n",
        "    if lineage_group_id:\n",
        "        cleaned_df['lineage_group_id'] = lineage_group_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "190ad4ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.pipeline.utils.cleaning_functions import coercion as coercion_mod\n",
        "\n",
        "coerced_data = []\n",
        "for df in cleaned_data:\n",
        "    # Example: coerce columns into the designated data types (int, float, datetime, geom, etc)\n",
        "    coerced_df = coercion_mod.coerce_columns(df,\n",
        "                                             int_cols=['repl_no', 'qty'], \n",
        "                                             float_cols=['value', 'particle_width', 'particle_length', 'particle_height'],\n",
        "                                             datetime_cols=['created_at', 'updated_at', 'fv_date_time', 'sample_ts', 'prod_date'])\n",
        "    coerced_data.append(coerced_df)\n",
        "\n",
        "print(f\"Coerced {len(coerced_data)} dataframes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032f93cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.pipeline.utils.name_id_swap import normalize_dataframes\n",
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import *\n",
        "\n",
        "normalize_columns = {\n",
        "    'resource': (Resource, 'name'),\n",
        "    'provider_codename': (Provider, 'codename'),\n",
        "    'primary_collector': (Contact, 'name'),\n",
        "    'storage_dur_units': (Unit, 'name'),\n",
        "    'particle_units': (Unit, 'name'),\n",
        "    'sample_unit': (Unit, 'name'),\n",
        "    'prepared_sample': (PreparedSample, 'name'),\n",
        "    'soil_type': (SoilType, 'name'),\n",
        "    'storage_mode': (Method, 'name'),\n",
        "    'county': (LocationAddress, 'county'),\n",
        "    'primary_ag_product': (PrimaryAgProduct, 'name'),\n",
        "    'provider_type': (Provider, 'type'),\n",
        "    'dataset': (Dataset, 'name'),\n",
        "    'field_storage_location' : (LocationAddress, 'full_address'),\n",
        "}\n",
        "\n",
        "normalized_data = []\n",
        "for df in coerced_data:\n",
        "    normalized_df = normalize_dataframes(df, normalize_columns)\n",
        "    normalized_data.append(normalized_df)\n",
        "\n",
        "normalized_data[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823f756f",
      "metadata": {},
      "outputs": [],
      "source": [
        "joined_data = normalized_data[0].merge(\n",
        "    normalized_data[1], \n",
        "    on='provider_codename_id', \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "joined_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2def234",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "##Select and Rename Columns\n",
        "\n",
        "# 1. Define the mapping of existing columns to new names\n",
        "rename_map = {\n",
        "    'field_sample_name': 'name',\n",
        "    'resource_id': 'resource_id',\n",
        "    'provider_codename_id': 'provider_id',\n",
        "    'primary_collector_id': 'collector_id',\n",
        "    'sample_source': 'sample_collection_source',\n",
        "    'qty': 'qty',\n",
        "    'sample_unit_id': 'amount_collected_unit_id',\n",
        "    'county_id': 'sampling_location_id',\n",
        "    'storage_mode_id': 'field_storage_method_id',\n",
        "    'storage_dur_value': 'field_storage_duration_value',\n",
        "    'storage_dur_units_id': 'field_storage_duration_unit_id',\n",
        "    'field_storage_location_id': 'field_storage_location_id',\n",
        "    'sample_ts': 'collection_timestamp',\n",
        "    'sample_notes': 'note'\n",
        "}\n",
        "\n",
        "# 2. Select existing columns, rename them, and assign new empty columns\n",
        "field_sample = joined_data[list(rename_map.keys())].rename(columns=rename_map).assign(\n",
        "    collection_method=None,\n",
        "    harvest_datemethod=None,\n",
        "    harvest_date=None,\n",
        "    field_sample_storage_location_id_2=None\n",
        ")\n",
        "\n",
        "field_sample.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145e4834",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Prepare Record Information DataFrames\n",
        "\n",
        "record_data = []\n",
        "for df in normalized_data:\n",
        "    # 1. Define explicit mappings for non-normalized columns\n",
        "    rename_map = {\n",
        "        'record_id': 'record_id',\n",
        "        'repl_no': 'technical_replication_no',\n",
        "        'qc_result': 'qc_pass',\n",
        "        'note': 'note'\n",
        "    }\n",
        "    \n",
        "    # 2. Dynamically add normalized columns from the normalize_columns dictionary\n",
        "    for col in normalize_columns.keys():\n",
        "        norm_col = f\"{col}_id\"\n",
        "        if norm_col in df.columns:\n",
        "            # Special case: rename to match target record table schema\n",
        "            target_name = 'analyst_id' if col == 'analyst_email' else \\\n",
        "                          'method_id' if col == 'preparation_method' else norm_col\n",
        "            rename_map[norm_col] = target_name\n",
        "\n",
        "    # 3. Only select columns that actually exist in this specific dataframe\n",
        "    available_cols = [c for c in rename_map.keys() if c in df.columns]\n",
        "    final_rename = {k: v for k, v in rename_map.items() if k in available_cols}\n",
        "\n",
        "    record_df = df[available_cols].copy().rename(columns=final_rename)\n",
        "\n",
        "    # 4. Drop rows where critical identifiers are missing (NaN)\n",
        "    if 'record_id' in record_df.columns:\n",
        "        record_df = record_df.dropna(subset=['record_id'])\n",
        "    \n",
        "    record_data.append(record_df)\n",
        "\n",
        "print(f\"Prepared {len(record_data)} record dataframes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d92bf7",
      "metadata": {},
      "source": [
        "## Load\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cf570b",
      "metadata": {},
      "outputs": [],
      "source": [
        "db_url = settings.database_url\n",
        "if \"@db:\" in db_url:\n",
        "    db_url = db_url.replace(\"@db:\", \"@localhost:\")\n",
        "elif \"db:5432\" in db_url:\n",
        "    db_url = db_url.replace(\"db:5432\", \"localhost:5432\")\n",
        "\n",
        "engine = create_engine(db_url)\n",
        "\n",
        "def upsert_observations(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        record['updated_at'] = now\n",
        "        if record.get('created_at') is None:\n",
        "            record['created_at'] = now\n",
        "        stmt = insert(Observation).values(record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in Observation.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    for obs_df in observation_data:\n",
        "        upsert_observations(obs_df, session)\n",
        "    session.commit()\n",
        "print('Upsert of all observations completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upsert-proximate",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import ProximateRecord\n",
        "\n",
        "def upsert_proximate_records(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    # Filter record dictionary to only include columns that exist in the table\n",
        "    table_columns = {c.name for c in ProximateRecord.__table__.columns}\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
        "        clean_record['updated_at'] = now\n",
        "        if clean_record.get('created_at') is None:\n",
        "            clean_record['created_at'] = now\n",
        "        stmt = insert(ProximateRecord).values(clean_record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in ProximateRecord.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    # Assuming the first dataframe in record_data is Proximate\n",
        "    upsert_proximate_records(record_data[0], session)\n",
        "    session.commit()\n",
        "print('Upsert of Proximate records completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upsert-ultimate",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import UltimateRecord\n",
        "\n",
        "def upsert_ultimate_records(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    table_columns = {c.name for c in UltimateRecord.__table__.columns}\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
        "        clean_record['updated_at'] = now\n",
        "        if clean_record.get('created_at') is None:\n",
        "            clean_record['created_at'] = now\n",
        "        stmt = insert(UltimateRecord).values(clean_record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in UltimateRecord.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    # Assuming the second dataframe in record_data is Ultimate\n",
        "    upsert_ultimate_records(record_data[1], session)\n",
        "    session.commit()\n",
        "print('Upsert of Ultimate records completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upsert-compositional",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.datamodels.schemas.generated.ca_biositing import CompositionalRecord\n",
        "\n",
        "def upsert_compositional_records(df, session):\n",
        "    if df.empty:\n",
        "        return\n",
        "    now = datetime.now(timezone.utc)\n",
        "    table_columns = {c.name for c in CompositionalRecord.__table__.columns}\n",
        "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
        "    for record in records:\n",
        "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
        "        clean_record['updated_at'] = now\n",
        "        if clean_record.get('created_at') is None:\n",
        "            clean_record['created_at'] = now\n",
        "        stmt = insert(CompositionalRecord).values(clean_record)\n",
        "        update_dict = {\n",
        "            c.name: stmt.excluded[c.name]\n",
        "            for c in CompositionalRecord.__table__.columns\n",
        "            if c.name not in ['id', 'created_at', 'record_id']\n",
        "        }\n",
        "        upsert_stmt = stmt.on_conflict_do_update(\n",
        "            index_elements=['record_id'],\n",
        "            set_=update_dict\n",
        "        )\n",
        "        session.execute(upsert_stmt)\n",
        "\n",
        "with Session(engine) as session:\n",
        "    # Assuming the third dataframe in record_data is Compositional\n",
        "    upsert_compositional_records(record_data[2], session)\n",
        "    session.commit()\n",
        "print('Upsert of Compositional records completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2833a54e",
      "metadata": {},
      "outputs": [],
      "source": [
        "feedstock_collector_info_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417a5d4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ca_biositing.pipeline.etl.extract.biodiesel_plants import extract as biodiesel_extract\n",
        "biodiesel_plants_raw = biodiesel_extract()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b8f359",
      "metadata": {},
      "outputs": [],
      "source": [
        "biodiesel_plants_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0370d001",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from ca_biositing.pipeline.etl.extract.basic_sample_info import extract as basic_sample_info_extract\n",
        "from ca_biositing.pipeline.etl.transform.resource import transform as resource_transform\n",
        "\n",
        "basic_sample_info = basic_sample_info_extract()\n",
        "\n",
        "EXTRACT_SOURCES: List[str] = [\"basic_sample_info\"]\n",
        "\n",
        "cleaned_data = resource_transform.fn({\"basic_sample_info\": basic_sample_info})\n",
        "\n",
        "cleaned_data.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ca-biositing (Pixi)",
      "language": "python",
      "name": "ca-biositing-pixi"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
