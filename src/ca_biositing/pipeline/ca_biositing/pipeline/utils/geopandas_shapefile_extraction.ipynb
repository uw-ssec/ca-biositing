{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geopandas Shapefile Extraction\n",
    "\n",
    "Utility notebook for loading a shapefile with **geopandas**, performing a simple inspection, and exporting data to a convenient format (e.g., CSV). This can be used as a starting point for any geospatial data preprocessing within the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install geopandas in the current environment if needed\n",
    "# !pixi add --feature gis --pypi geopandas\n",
    "# (Uncomment the line above and run the cell in the appropriate environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f22c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DataStatus UniqueID DWR_REVISE SYMB_CLASS MULTIUSE CLASS1 SUBCLASS1  \\\n",
      "0  Provisional  4900289       None          G        S     **        **   \n",
      "1  Provisional  4900338       None          G        S     **        **   \n",
      "2  Provisional  4901032       None          G        S     **        **   \n",
      "3  Provisional  4901050       None          G        S     **        **   \n",
      "4  Provisional  4901182       None          G        D      V        **   \n",
      "\n",
      "  SPECOND1 IRR_TYP1PA IRR_TYP1PB  ... REGION     ACRES  COUNTY    HYDRO_RGN  \\\n",
      "0        *          *          *  ...   NCRO  0.554073  Sonoma  North Coast   \n",
      "1        *          *          *  ...   NCRO  3.381716  Sonoma  North Coast   \n",
      "2        *          *          *  ...   NCRO  1.222060  Sonoma  North Coast   \n",
      "3        *          *          *  ...   NCRO  1.247221  Sonoma  North Coast   \n",
      "4        *          *          *  ...   NCRO  0.810619  Sonoma  North Coast   \n",
      "\n",
      "            LIQ_REPORT MAIN_CROP MAIN_CROP_ Shape_Leng    Shape_Area  \\\n",
      "0  G6   **** **** ****        G6      118.0   0.002002  2.310881e-07   \n",
      "1  G6   **** **** ****        G6       20.0   0.004707  1.411760e-06   \n",
      "2  G6   **** **** ****        G6       86.0   0.004302  5.102894e-07   \n",
      "3  G6   **** **** ****        G6      126.0   0.003350  5.227858e-07   \n",
      "4  V    G6   **** ****        G6      100.0   0.002245  3.382431e-07   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON Z ((-122.98063 38.35045 0, -122.98068 ...  \n",
      "1  POLYGON Z ((-122.90137 38.42115 0, -122.90132 ...  \n",
      "2  POLYGON Z ((-122.7868 38.43781 0, -122.78679 3...  \n",
      "3  POLYGON Z ((-122.84651 38.71665 0, -122.84545 ...  \n",
      "4  POLYGON Z ((-122.92076 38.38457 0, -122.92096 ...  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "CRS: EPSG:4269\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\n",
    "# Path to your shapefile (update this path to point at the desired .shp file)\n",
    "#shapefile_path = 'data/example_shapefile.shp'\n",
    "\n",
    "# This is the path to the landiq dataset saved locally on Peter's machine\n",
    "shapefile_path = '/Users/pjsmitty301/BioCirV/i15_crop_mapping_2023_provisional'\n",
    "\n",
    "# Load the shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Quick inspection\n",
    "print(gdf.head())\n",
    "print(f'CRS: {gdf.crs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "452d8be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "43618eb9-8be9-43a7-b8dd-c51fb8f80591",
       "rows": [
        [
         "DataStatus",
         "object"
        ],
        [
         "UniqueID",
         "object"
        ],
        [
         "DWR_REVISE",
         "object"
        ],
        [
         "SYMB_CLASS",
         "object"
        ],
        [
         "MULTIUSE",
         "object"
        ],
        [
         "CLASS1",
         "object"
        ],
        [
         "SUBCLASS1",
         "object"
        ],
        [
         "SPECOND1",
         "object"
        ],
        [
         "IRR_TYP1PA",
         "object"
        ],
        [
         "IRR_TYP1PB",
         "object"
        ],
        [
         "PCNT1",
         "object"
        ],
        [
         "CLASS2",
         "object"
        ],
        [
         "SUBCLASS2",
         "object"
        ],
        [
         "SPECOND2",
         "object"
        ],
        [
         "IRR_TYP2PA",
         "object"
        ],
        [
         "IRR_TYP2PB",
         "object"
        ],
        [
         "PCNT2",
         "object"
        ],
        [
         "CLASS3",
         "object"
        ],
        [
         "SUBCLASS3",
         "object"
        ],
        [
         "SPECOND3",
         "object"
        ],
        [
         "IRR_TYP3PA",
         "object"
        ],
        [
         "IRR_TYP3PB",
         "object"
        ],
        [
         "PCNT3",
         "object"
        ],
        [
         "CLASS4",
         "object"
        ],
        [
         "SUBCLASS4",
         "object"
        ],
        [
         "SPECOND4",
         "object"
        ],
        [
         "IRR_TYP4PA",
         "object"
        ],
        [
         "IRR_TYP4PB",
         "object"
        ],
        [
         "PCNT4",
         "object"
        ],
        [
         "UCF_ATT",
         "object"
        ],
        [
         "YR_PLANTED",
         "int64"
        ],
        [
         "SEN_CROP",
         "object"
        ],
        [
         "ADOY_SEN",
         "float64"
        ],
        [
         "CROPTYP1",
         "object"
        ],
        [
         "CTYP1_NOTE",
         "object"
        ],
        [
         "ADOY1",
         "float64"
        ],
        [
         "CROPTYP2",
         "object"
        ],
        [
         "CTYP2_NOTE",
         "object"
        ],
        [
         "ADOY2",
         "float64"
        ],
        [
         "CROPTYP3",
         "object"
        ],
        [
         "CTYP3_NOTE",
         "object"
        ],
        [
         "ADOY3",
         "float64"
        ],
        [
         "CROPTYP4",
         "object"
        ],
        [
         "CTYP4_NOTE",
         "object"
        ],
        [
         "ADOY4",
         "float64"
        ],
        [
         "EMRG_CROP",
         "object"
        ],
        [
         "ADOY_EMRG",
         "float64"
        ],
        [
         "REGION",
         "object"
        ],
        [
         "ACRES",
         "float64"
        ],
        [
         "COUNTY",
         "object"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 57
       }
      },
      "text/plain": [
       "DataStatus      object\n",
       "UniqueID        object\n",
       "DWR_REVISE      object\n",
       "SYMB_CLASS      object\n",
       "MULTIUSE        object\n",
       "CLASS1          object\n",
       "SUBCLASS1       object\n",
       "SPECOND1        object\n",
       "IRR_TYP1PA      object\n",
       "IRR_TYP1PB      object\n",
       "PCNT1           object\n",
       "CLASS2          object\n",
       "SUBCLASS2       object\n",
       "SPECOND2        object\n",
       "IRR_TYP2PA      object\n",
       "IRR_TYP2PB      object\n",
       "PCNT2           object\n",
       "CLASS3          object\n",
       "SUBCLASS3       object\n",
       "SPECOND3        object\n",
       "IRR_TYP3PA      object\n",
       "IRR_TYP3PB      object\n",
       "PCNT3           object\n",
       "CLASS4          object\n",
       "SUBCLASS4       object\n",
       "SPECOND4        object\n",
       "IRR_TYP4PA      object\n",
       "IRR_TYP4PB      object\n",
       "PCNT4           object\n",
       "UCF_ATT         object\n",
       "YR_PLANTED       int64\n",
       "SEN_CROP        object\n",
       "ADOY_SEN       float64\n",
       "CROPTYP1        object\n",
       "CTYP1_NOTE      object\n",
       "ADOY1          float64\n",
       "CROPTYP2        object\n",
       "CTYP2_NOTE      object\n",
       "ADOY2          float64\n",
       "CROPTYP3        object\n",
       "CTYP3_NOTE      object\n",
       "ADOY3          float64\n",
       "CROPTYP4        object\n",
       "CTYP4_NOTE      object\n",
       "ADOY4          float64\n",
       "EMRG_CROP       object\n",
       "ADOY_EMRG      float64\n",
       "REGION          object\n",
       "ACRES          float64\n",
       "COUNTY          object\n",
       "HYDRO_RGN       object\n",
       "LIQ_REPORT      object\n",
       "MAIN_CROP       object\n",
       "MAIN_CROP_     float64\n",
       "Shape_Leng     float64\n",
       "Shape_Area     float64\n",
       "geometry      geometry\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fe633f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from prefect import task, get_run_logger\n",
    "import ca_biositing.pipeline.utils.cleaning_functions.cleaning as cleaning_mod\n",
    "import ca_biositing.pipeline.utils.cleaning_functions.coercion as coercion_mod\n",
    "from ca_biositing.pipeline.utils.name_id_swap import normalize_dataframes\n",
    "\n",
    "@task\n",
    "def transform_landiq_record(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    etl_run_id: str = None,\n",
    "    lineage_group_id: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms Land IQ GeoDataFrame into the LandiqRecord table format.\n",
    "\n",
    "    Args:\n",
    "        gdf: Raw GeoDataFrame from Land IQ shapefile.\n",
    "        etl_run_id: ID of the current ETL run.\n",
    "        lineage_group_id: ID of the lineage group.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame formatted for the landiq_record table.\n",
    "    \"\"\"\n",
    "    from ca_biositing.datamodels.schemas.generated.ca_biositing import (\n",
    "        Dataset,\n",
    "        Polygon,\n",
    "        PrimaryAgProduct,\n",
    "    )\n",
    "\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Transforming Land IQ data for LandiqRecord table\")\n",
    "\n",
    "    if gdf is None or gdf.empty:\n",
    "        logger.error(\"Input GeoDataFrame is empty or None\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 1. Initial Cleaning & Preparation\n",
    "    # Convert GeoDataFrame to regular DataFrame to avoid issues with standard_clean\n",
    "    df = pd.DataFrame(gdf.copy())\n",
    "\n",
    "    # Set dataset name and version as requested\n",
    "    df['dataset'] = 'landiq'\n",
    "    df['version'] = 'land use 2023'\n",
    "\n",
    "    # Map shapefile columns to model fields\n",
    "    # MAIN_CROP is the main crop for single cropped fields\n",
    "    if 'MAIN_CROP' in df.columns:\n",
    "        df['main_crop'] = df['MAIN_CROP']\n",
    "    if 'CLASS1' in df.columns:\n",
    "        df['secondary_crop'] = df['CLASS1']\n",
    "    if 'CLASS2' in df.columns:\n",
    "        df['tertiary_crop'] = df['CLASS2']\n",
    "    if 'CLASS3' in df.columns:\n",
    "        df['quaternary_crop'] = df['CLASS3']\n",
    "\n",
    "    # Map Confidence to confidence\n",
    "    if 'CONFIDENCE' in df.columns:\n",
    "        df['confidence'] = df['CONFIDENCE']\n",
    "\n",
    "    # Load crop mapping\n",
    "    # Load crop mapping\n",
    "    try:\n",
    "        mapping_path = os.path.join(os.path.dirname(__file__), 'crops_classification.csv')\n",
    "        mapping_df = pd.read_csv(mapping_path)\n",
    "        crop_map = {str(k).strip().upper(): v for k, v in zip(mapping_df['crop_code'], mapping_df['crop'])}\n",
    "        logger.info(f\"Loaded {len(crop_map)} crop mappings from {mapping_path}\")\n",
    "\n",
    "        # Convert crop codes to text\n",
    "        for col in ['main_crop', 'secondary_crop', 'tertiary_crop', 'quaternary_crop']:\n",
    "            if col in df.columns:\n",
    "                # Ensure we handle potential whitespace and case sensitivity in codes\n",
    "                df[col] = df[col].astype(str).str.strip().str.upper().map(crop_map).fillna(df[col])\n",
    "    except Exception as e:\n",
    "        # Use print as fallback if logger isn't initialized in some contexts\n",
    "        msg = f\"Could not load or apply crop mapping: {e}\"\n",
    "        try:\n",
    "            logger.warning(msg)\n",
    "        except:\n",
    "            print(msg)\n",
    "\n",
    "    # Map UniqueID to record_id for lineage and upsert\n",
    "    if 'UniqueID' in df.columns:\n",
    "        df['record_id'] = df['UniqueID']\n",
    "    elif 'UNIQUEID' in df.columns:\n",
    "        df['record_id'] = df['UNIQUEID']\n",
    "\n",
    "    # Handle Irrigation status (IRR_TYP1PA/IRR_TYP2PA etc)\n",
    "    if 'IRR_TYP1PA' in df.columns:\n",
    "        df['irrigated'] = df['IRR_TYP1PA'].astype(str).str.lower().str.contains('irrigated')\n",
    "    else:\n",
    "        df['irrigated'] = False\n",
    "\n",
    "    # 2. Standard Clean\n",
    "    # We pass lowercase=False because standard_clean's to_lowercase_df implementation\n",
    "    # has a bug where it tries to access .str on the DataFrame itself if columns is None.\n",
    "    # 2. Standard Clean\n",
    "    # We pass lowercase=False and replace_empty=False to avoid bugs in cleaning.py\n",
    "    # that occur when processing DataFrames with certain column types.\n",
    "    # Ensure crop columns are preserved after clean_names_df\n",
    "    # We do this by explicitly passing them to clean_names_df if it supports it,\n",
    "    # or re-adding them after.\n",
    "    cleaned_df = cleaning_mod.clean_names_df(df)\n",
    "    \n",
    "    # Remove duplicate columns if any (e.g., if 'main_crop' already existed)\n",
    "    # We do this BEFORE re-applying mapping to avoid 'DataFrame object has no attribute str'\n",
    "    cleaned_df = cleaned_df.loc[:, ~cleaned_df.columns.duplicated()].copy()\n",
    "\n",
    "    # Re-apply mapping to the cleaned dataframe to ensure correct values\n",
    "    for col in ['main_crop', 'secondary_crop', 'tertiary_crop', 'quaternary_crop']:\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_df[col] = cleaned_df[col].astype(str).str.strip().str.upper().map(crop_map).fillna(cleaned_df[col])\n",
    "\n",
    "    # Manually lowercase string columns and handle empty strings\n",
    "    # We iterate over columns and check if they are string-like to avoid AttributeError\n",
    "    for i in range(len(cleaned_df.columns)):\n",
    "        # Use iloc with integer index to handle potential duplicate column names\n",
    "        # which can cause .loc to return a DataFrame instead of a Series\n",
    "        series = cleaned_df.iloc[:, i]\n",
    "\n",
    "        if series.dtype == \"object\" or pd.api.types.is_string_dtype(series):\n",
    "            # Use Series-level .str accessor explicitly\n",
    "            cleaned_df.iloc[:, i] = series.astype(str).str.lower().replace(r\"^\\s*$\", None, regex=True)\n",
    "\n",
    "    # Add lineage IDs\n",
    "    if etl_run_id:\n",
    "        cleaned_df['etl_run_id'] = etl_run_id\n",
    "    if lineage_group_id:\n",
    "        cleaned_df['lineage_group_id'] = lineage_group_id\n",
    "\n",
    "    # 3. Coercion\n",
    "    coerced_df = coercion_mod.coerce_columns(\n",
    "        cleaned_df,\n",
    "        float_cols=['acres'],\n",
    "        int_cols=['confidence'] if 'confidence' in cleaned_df.columns else []\n",
    "    )\n",
    "\n",
    "    # 4. Normalization\n",
    "    # We need to map names to IDs for related tables\n",
    "    # We also normalize polygons using the geometry (WKT) as the identifier\n",
    "    normalize_columns = {\n",
    "        'dataset': (Dataset, 'name'),\n",
    "        'main_crop': (PrimaryAgProduct, 'name'),\n",
    "        'secondary_crop': (PrimaryAgProduct, 'name'),\n",
    "        'tertiary_crop': (PrimaryAgProduct, 'name'),\n",
    "        'quaternary_crop': (PrimaryAgProduct, 'name'),\n",
    "        'geometry': (Polygon, 'geom'),\n",
    "    }\n",
    "\n",
    "    # Ensure geometry is in WKT format for normalization if it's a GeoSeries\n",
    "    if 'geometry' in coerced_df.columns and hasattr(coerced_df['geometry'], 'to_wkt'):\n",
    "        coerced_df['geometry'] = coerced_df['geometry'].to_wkt()\n",
    "\n",
    "    normalized_df = normalize_dataframes(coerced_df, normalize_columns)\n",
    "\n",
    "    # 5. Table Specific Mapping\n",
    "    rename_map = {\n",
    "        'record_id': 'record_id',\n",
    "        'acres': 'acres',\n",
    "        'version': 'version',\n",
    "        'etl_run_id': 'etl_run_id',\n",
    "        'lineage_group_id': 'lineage_group_id',\n",
    "        'irrigated': 'irrigated',\n",
    "        'confidence': 'confidence'\n",
    "    }\n",
    "\n",
    "    # Add normalized ID columns\n",
    "    for col in normalize_columns.keys():\n",
    "        norm_col = f\"{col}_id\"\n",
    "        if norm_col in normalized_df.columns:\n",
    "            # Special case: geometry_id maps to polygon_id in LandiqRecord\n",
    "            target_col = 'polygon_id' if col == 'geometry' else norm_col\n",
    "            rename_map[norm_col] = target_col\n",
    "\n",
    "    # Ensure dataset_id is included if it was normalized\n",
    "    if 'dataset_id' in normalized_df.columns:\n",
    "        rename_map['dataset_id'] = 'dataset_id'\n",
    "\n",
    "    # Ensure crop columns are preserved if they were normalized\n",
    "    # We map the normalized ID columns (e.g., main_crop_id) back to the\n",
    "    # model field names (e.g., main_crop) expected by the database.\n",
    "    for col in ['main_crop', 'secondary_crop', 'tertiary_crop', 'quaternary_crop']:\n",
    "        norm_col = f\"{col}_id\"\n",
    "        if norm_col in normalized_df.columns:\n",
    "            rename_map[norm_col] = col\n",
    "\n",
    "    available_cols = [c for c in rename_map.keys() if c in normalized_df.columns]\n",
    "    final_rename = {k: v for k, v in rename_map.items() if k in available_cols}\n",
    "\n",
    "    try:\n",
    "        record_df = normalized_df[available_cols].copy().rename(columns=final_rename)\n",
    "\n",
    "        # Ensure record_id exists for lineage tracking\n",
    "        if 'record_id' in record_df.columns:\n",
    "            record_df = record_df.dropna(subset=['record_id'])\n",
    "        else:\n",
    "            logger.warning(\"record_id (UniqueID) missing from Land IQ transform\")\n",
    "\n",
    "        # Add geometry for polygon handling in load step\n",
    "        if 'geometry' in gdf.columns:\n",
    "            record_df['geometry'] = gdf['geometry'].values\n",
    "\n",
    "        logger.info(f\"Successfully transformed {len(record_df)} Land IQ records\")\n",
    "        return record_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during Land IQ transform: {e}\", exc_info=True)\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c6af07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">08:30:14.783 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'transform_landiq_record' - Transforming Land IQ data for LandiqRecord table\n",
       "</pre>\n"
      ],
      "text/plain": [
       "08:30:14.783 | \u001b[36mINFO\u001b[0m    | Task run 'transform_landiq_record' - Transforming Land IQ data for LandiqRecord table\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">08:30:14.906 | <span style=\"color: #d7d700; text-decoration-color: #d7d700\">WARNING</span> | Task run 'transform_landiq_record' - Could not load or apply crop mapping: name '__file__' is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "08:30:14.906 | \u001b[38;5;184mWARNING\u001b[0m | Task run 'transform_landiq_record' - Could not load or apply crop mapping: name '__file__' is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">08:30:16.002 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'transform_landiq_record' - Task run failed with exception: UnboundLocalError(\"cannot access local variable 'crop_map' where it is not associated with a value\")\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/pjsmitty301/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py\", line 884, in run_context\n",
       "    yield self\n",
       "  File \"/Users/pjsmitty301/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py\", line 1538, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "    ~~~~~~~~~~~~~~~~~~~^^^^^\n",
       "  File \"/Users/pjsmitty301/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py\", line 901, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "  File \"/Users/pjsmitty301/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "  File \"/var/folders/2l/qpqn5_6578z142wxn32lbtw00000gn/T/ipykernel_98363/2783163512.py\", line 113, in transform_landiq_record\n",
       "    cleaned_df[col] = cleaned_df[col].astype(str).str.strip().str.upper().map(crop_map).fillna(cleaned_df[col])\n",
       "                                                                              ^^^^^^^^\n",
       "UnboundLocalError: cannot access local variable 'crop_map' where it is not associated with a value\n",
       "</pre>\n"
      ],
      "text/plain": [
       "08:30:16.002 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'transform_landiq_record' - Task run failed with exception: UnboundLocalError(\"cannot access local variable 'crop_map' where it is not associated with a value\")\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/pjsmitty301/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py\", line 884, in run_context\n",
       "    yield self\n",
       "  File \"/Users/pjsmitty301/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py\", line 1538, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "    ~~~~~~~~~~~~~~~~~~~^^^^^\n",
       "  File \"/Users/pjsmitty301/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py\", line 901, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "  File \"/Users/pjsmitty301/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/utilities/callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "  File \"/var/folders/2l/qpqn5_6578z142wxn32lbtw00000gn/T/ipykernel_98363/2783163512.py\", line 113, in transform_landiq_record\n",
       "    cleaned_df[col] = cleaned_df[col].astype(str).str.strip().str.upper().map(crop_map).fillna(cleaned_df[col])\n",
       "                                                                              ^^^^^^^^\n",
       "UnboundLocalError: cannot access local variable 'crop_map' where it is not associated with a value\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">08:30:16.006 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'transform_landiq_record' - Finished in state <span style=\"color: #d70000; text-decoration-color: #d70000\">Failed</span>(\"Task run encountered an exception UnboundLocalError: cannot access local variable 'crop_map' where it is not associated with a value\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "08:30:16.006 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'transform_landiq_record' - Finished in state \u001b[38;5;160mFailed\u001b[0m(\"Task run encountered an exception UnboundLocalError: cannot access local variable 'crop_map' where it is not associated with a value\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'crop_map' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtransform_landiq_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/tasks.py:1147\u001b[39m, in \u001b[36mTask.__call__\u001b[39m\u001b[34m(self, return_state, wait_for, *args, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m track_viz_task(\n\u001b[32m   1142\u001b[39m         \u001b[38;5;28mself\u001b[39m.isasync, \u001b[38;5;28mself\u001b[39m.name, parameters, \u001b[38;5;28mself\u001b[39m.viz_return_value\n\u001b[32m   1143\u001b[39m     )\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprefect\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtask_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_task\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py:1765\u001b[39m, in \u001b[36mrun_task\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run_task_async(**kwargs)\n\u001b[32m   1764\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_task_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py:1540\u001b[39m, in \u001b[36mrun_task_sync\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1533\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1534\u001b[39m             engine.asset_context(),\n\u001b[32m   1535\u001b[39m             engine.run_context(),\n\u001b[32m   1536\u001b[39m             engine.transaction_context() \u001b[38;5;28;01mas\u001b[39;00m txn,\n\u001b[32m   1537\u001b[39m         ):\n\u001b[32m   1538\u001b[39m             engine.call_task_fn(txn)\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py:499\u001b[39m, in \u001b[36mSyncTaskRunEngine.result\u001b[39m\u001b[34m(self, raise_on_failure)\u001b[39m\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotSet:\n\u001b[32m    497\u001b[39m     \u001b[38;5;66;03m# if the task raised an exception, raise it\u001b[39;00m\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n\u001b[32m    501\u001b[39m     \u001b[38;5;66;03m# otherwise, return the exception\u001b[39;00m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raised\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py:884\u001b[39m, in \u001b[36mSyncTaskRunEngine.run_context\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    881\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_cancelled():\n\u001b[32m    882\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m CancelledError(\u001b[33m\"\u001b[39m\u001b[33mTask run cancelled by the task runner\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m884\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    886\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_timeout(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py:1538\u001b[39m, in \u001b[36mrun_task_sync\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1532\u001b[39m         run_coro_as_sync(engine.wait_until_ready())\n\u001b[32m   1533\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   1534\u001b[39m             engine.asset_context(),\n\u001b[32m   1535\u001b[39m             engine.run_context(),\n\u001b[32m   1536\u001b[39m             engine.transaction_context() \u001b[38;5;28;01mas\u001b[39;00m txn,\n\u001b[32m   1537\u001b[39m         ):\n\u001b[32m-> \u001b[39m\u001b[32m1538\u001b[39m             \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_task_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/task_engine.py:901\u001b[39m, in \u001b[36mSyncTaskRunEngine.call_task_fn\u001b[39m\u001b[34m(self, transaction)\u001b[39m\n\u001b[32m    899\u001b[39m     result = transaction.read()\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m     result = \u001b[43mcall_with_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28mself\u001b[39m.handle_success(result, transaction=transaction)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ca-biositing/.pixi/envs/default/lib/python3.13/site-packages/prefect/utilities/callables.py:210\u001b[39m, in \u001b[36mcall_with_parameters\u001b[39m\u001b[34m(fn, parameters)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03mCall a function with parameters extracted with `get_call_parameters`\u001b[39;00m\n\u001b[32m    204\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m \u001b[33;03mthe args/kwargs using `parameters_to_positional_and_keyword` directly\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m args, kwargs = parameters_to_args_kwargs(fn, parameters)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mtransform_landiq_record\u001b[39m\u001b[34m(gdf, etl_run_id, lineage_group_id)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mmain_crop\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msecondary_crop\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtertiary_crop\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mquaternary_crop\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cleaned_df.columns:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         cleaned_df[col] = cleaned_df[col].astype(\u001b[38;5;28mstr\u001b[39m).str.strip().str.upper().map(\u001b[43mcrop_map\u001b[49m).fillna(cleaned_df[col])\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Manually lowercase string columns and handle empty strings\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# We iterate over columns and check if they are string-like to avoid AttributeError\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cleaned_df.columns)):\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# Use iloc with integer index to handle potential duplicate column names\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# which can cause .loc to return a DataFrame instead of a Series\u001b[39;00m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'crop_map' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "transform_landiq_record(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the geometry (simple quick visual)\n",
    "gdf.plot(figsize=(10, 6))\n",
    "plt.title('Shapefile Overview')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88dbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor as jn\n",
    "import logging\n",
    "from IPython.display import display\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import select\n",
    "\n",
    "# --- Basic Logging Configuration for Notebook ---\n",
    "# When running in a notebook, we use Python's standard logging.\n",
    "# In the production pipeline, this will be replaced by Prefect's `get_run_logger()`\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- Robustly find the project root ---\n",
    "# This ensures that the notebook can be run from any directory within the project.\n",
    "path = os.getcwd()\n",
    "project_root = None\n",
    "while path != os.path.dirname(path):\n",
    "    if 'pixi.toml' in os.listdir(path):\n",
    "        project_root = path\n",
    "        break\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "if not project_root:\n",
    "    raise FileNotFoundError(\"Could not find project root containing 'pixi.toml'.\")\n",
    "\n",
    "# Add the project root to the Python path to allow for module imports\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    logger.info(f\"Added project root '{project_root}' to sys.path\")\n",
    "else:\n",
    "    logger.info(f\"Project root '{project_root}' is already in sys.path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the refactored cleaning/coercion helpers from the new package\n",
    "from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.cleaning_functions import standard_clean, coerce_columns, coerce_columns_list\n",
    "\n",
    "def clean_the_gsheets(df, lowercase=True, replace_empty=True):\n",
    "    \"\"\"Wrapper that applies the standardized cleaning pipeline implemented in `cleaning_functions`.\"\"\"\n",
    "    logger.info('Starting DataFrame cleaning via standard_clean.')\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        logger.error('Input is not a pandas DataFrame.')\n",
    "        return None\n",
    "    try:\n",
    "        # Run the composed standard clean (names, empty->NA, lowercase, convert_dtypes)\n",
    "        df_cleaned = standard_clean(df, lowercase=lowercase, replace_empty=replace_empty)\n",
    "        # Preserve behaviour: drop rows missing key columns if present\n",
    "        subset = [c for c in ['resource', 'value'] if c in df_cleaned.columns]\n",
    "        if subset:\n",
    "            df_cleaned = df_cleaned.dropna(subset=subset)\n",
    "        logger.info(f'Cleaning complete; rows remaining: {len(df_cleaned)}')\n",
    "        return df_cleaned\n",
    "    except Exception as e:\n",
    "        logger.error(f'An error occurred during DataFrame cleaning: {e}', exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Coercion Configuration Templates ---\n",
    "# You can define column coercions in two ways: explicit keyword arguments or a dtype_map dictionary.\n",
    "# For geometry: use geopandas to load shapefiles (geometry column is already properly typed).\n",
    "# Only use geometry_cols if you have WKT strings to parse.\n",
    "\n",
    "# APPROACH 1: Explicit keyword arguments (clear and direct)\n",
    "COERCION_CONFIG_EXPLICIT = {\n",
    "    'int_cols': ['repl_no', 'sample_no'],\n",
    "    'float_cols': ['value', 'measurement'],\n",
    "    'datetime_cols': ['created_at', 'updated_at'],\n",
    "    'bool_cols': ['is_valid'],\n",
    "    'category_cols': ['status'],\n",
    "    'geometry_cols': []  # Use only if you have WKT strings; prefer geopandas for shapefiles\n",
    "}\n",
    "\n",
    "# APPROACH 2: dtype_map dictionary (compact, useful for dynamic configs)\n",
    "COERCION_CONFIG_DTYPE_MAP = {\n",
    "    'int': ['repl_no', 'sample_no'],\n",
    "    'float': ['value', 'measurement'],\n",
    "    'datetime': ['created_at', 'updated_at'],\n",
    "    'bool': ['is_valid'],\n",
    "    'category': ['status'],\n",
    "    'geometry': []  # Use only if you have WKT strings; prefer geopandas for shapefiles\n",
    "}\n",
    "\n",
    "# APPROACH 3: GeoPandas GeoDataFrame (for shapefiles and spatial data)\n",
    "# When loading shapefiles with geopandas, geometry is already a GeoSeries.\n",
    "# Use geometry_format='geodataframe' to skip coercion:\n",
    "GEOPANDAS_CONFIG = {\n",
    "    'int_cols': ['id', 'repl_no'],\n",
    "    'float_cols': ['area', 'value'],\n",
    "    'geometry_cols': ['geometry'],\n",
    "    'geometry_format': 'geodataframe'  # Don't convert; already properly typed\n",
    "}\n",
    "\n",
    "# Usage: coerce_the_gsheets(df, **COERCION_CONFIG_EXPLICIT)\n",
    "#   or: coerce_the_gsheets(df, dtype_map=COERCION_CONFIG_DTYPE_MAP)\n",
    "#   or: coerce_the_gsheets(gdf, **GEOPANDAS_CONFIG)  # for GeoDataFrames\n",
    "\n",
    "\n",
    "def coerce_the_gsheets(df, dtype_map=None, int_cols=None, float_cols=None, datetime_cols=None, bool_cols=None, category_cols=None, geometry_cols=None, geometry_format='wkt'):\n",
    "    \"\"\"Coerce specified columns on a cleaned DataFrame using coercion helpers.\n",
    "    `dtype_map` is an alternative mapping where keys are 'int','float','datetime','bool','category','geometry'.\n",
    "    `geometry_format` controls geometry coercion: 'wkt' (parse WKT strings) or 'geodataframe' (skip, already typed).\"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        logger.error('coerce_the_gsheets: input is not a DataFrame')\n",
    "        return df\n",
    "    return coerce_columns(df, int_cols=int_cols, float_cols=float_cols, datetime_cols=datetime_cols, bool_cols=bool_cols, category_cols=category_cols, geometry_cols=geometry_cols, dtype_map=dtype_map, geometry_format=geometry_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_the_gsheets(gdf).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export attributes to CSV for downstream processing (optional)\n",
    "output_csv = 'data/shapefile_attributes.csv'\n",
    "gdf.drop(columns='geometry').to_csv(output_csv, index=False)\n",
    "print(f'Attributes exported to {output_csv}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
