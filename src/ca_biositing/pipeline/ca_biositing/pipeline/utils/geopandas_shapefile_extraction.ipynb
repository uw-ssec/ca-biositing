{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geopandas Shapefile Extraction\n",
    "\n",
    "Utility notebook for loading a shapefile with **geopandas**, performing a simple inspection, and exporting data to a convenient format (e.g., CSV). This can be used as a starting point for any geospatial data preprocessing within the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install geopandas in the current environment if needed\n",
    "# !pixi add --feature gis --pypi geopandas\n",
    "# (Uncomment the line above and run the cell in the appropriate environment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\n",
    "# Path to your shapefile (update this path to point at the desired .shp file)\n",
    "#shapefile_path = 'data/example_shapefile.shp'\n",
    "\n",
    "shapefile_path = '/Users/pjsmitty301/BioCirV/i15_crop_mapping_2023_provisional'\n",
    "\n",
    "# Load the shapefile\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Quick inspection\n",
    "print(gdf.head())\n",
    "print(f'CRS: {gdf.crs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the geometry (simple quick visual)\n",
    "gdf.plot(figsize=(10, 6))\n",
    "plt.title('Shapefile Overview')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88dbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor as jn\n",
    "import logging\n",
    "from IPython.display import display\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import select\n",
    "\n",
    "# --- Basic Logging Configuration for Notebook ---\n",
    "# When running in a notebook, we use Python's standard logging.\n",
    "# In the production pipeline, this will be replaced by Prefect's `get_run_logger()`\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- Robustly find the project root ---\n",
    "# This ensures that the notebook can be run from any directory within the project.\n",
    "path = os.getcwd()\n",
    "project_root = None\n",
    "while path != os.path.dirname(path):\n",
    "    if 'pixi.toml' in os.listdir(path):\n",
    "        project_root = path\n",
    "        break\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "if not project_root:\n",
    "    raise FileNotFoundError(\"Could not find project root containing 'pixi.toml'.\")\n",
    "\n",
    "# Add the project root to the Python path to allow for module imports\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    logger.info(f\"Added project root '{project_root}' to sys.path\")\n",
    "else:\n",
    "    logger.info(f\"Project root '{project_root}' is already in sys.path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the refactored cleaning/coercion helpers from the new package\n",
    "from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.cleaning_functions import standard_clean, coerce_columns, coerce_columns_list\n",
    "\n",
    "def clean_the_gsheets(df, lowercase=True, replace_empty=True):\n",
    "    \"\"\"Wrapper that applies the standardized cleaning pipeline implemented in `cleaning_functions`.\"\"\"\n",
    "    logger.info('Starting DataFrame cleaning via standard_clean.')\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        logger.error('Input is not a pandas DataFrame.')\n",
    "        return None\n",
    "    try:\n",
    "        # Run the composed standard clean (names, empty->NA, lowercase, convert_dtypes)\n",
    "        df_cleaned = standard_clean(df, lowercase=lowercase, replace_empty=replace_empty)\n",
    "        # Preserve behaviour: drop rows missing key columns if present\n",
    "        subset = [c for c in ['resource', 'value'] if c in df_cleaned.columns]\n",
    "        if subset:\n",
    "            df_cleaned = df_cleaned.dropna(subset=subset)\n",
    "        logger.info(f'Cleaning complete; rows remaining: {len(df_cleaned)}')\n",
    "        return df_cleaned\n",
    "    except Exception as e:\n",
    "        logger.error(f'An error occurred during DataFrame cleaning: {e}', exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Coercion Configuration Templates ---\n",
    "# You can define column coercions in two ways: explicit keyword arguments or a dtype_map dictionary.\n",
    "# For geometry: use geopandas to load shapefiles (geometry column is already properly typed).\n",
    "# Only use geometry_cols if you have WKT strings to parse.\n",
    "\n",
    "# APPROACH 1: Explicit keyword arguments (clear and direct)\n",
    "COERCION_CONFIG_EXPLICIT = {\n",
    "    'int_cols': ['repl_no', 'sample_no'],\n",
    "    'float_cols': ['value', 'measurement'],\n",
    "    'datetime_cols': ['created_at', 'updated_at'],\n",
    "    'bool_cols': ['is_valid'],\n",
    "    'category_cols': ['status'],\n",
    "    'geometry_cols': []  # Use only if you have WKT strings; prefer geopandas for shapefiles\n",
    "}\n",
    "\n",
    "# APPROACH 2: dtype_map dictionary (compact, useful for dynamic configs)\n",
    "COERCION_CONFIG_DTYPE_MAP = {\n",
    "    'int': ['repl_no', 'sample_no'],\n",
    "    'float': ['value', 'measurement'],\n",
    "    'datetime': ['created_at', 'updated_at'],\n",
    "    'bool': ['is_valid'],\n",
    "    'category': ['status'],\n",
    "    'geometry': []  # Use only if you have WKT strings; prefer geopandas for shapefiles\n",
    "}\n",
    "\n",
    "# APPROACH 3: GeoPandas GeoDataFrame (for shapefiles and spatial data)\n",
    "# When loading shapefiles with geopandas, geometry is already a GeoSeries.\n",
    "# Use geometry_format='geodataframe' to skip coercion:\n",
    "GEOPANDAS_CONFIG = {\n",
    "    'int_cols': ['id', 'repl_no'],\n",
    "    'float_cols': ['area', 'value'],\n",
    "    'geometry_cols': ['geometry'],\n",
    "    'geometry_format': 'geodataframe'  # Don't convert; already properly typed\n",
    "}\n",
    "\n",
    "# Usage: coerce_the_gsheets(df, **COERCION_CONFIG_EXPLICIT)\n",
    "#   or: coerce_the_gsheets(df, dtype_map=COERCION_CONFIG_DTYPE_MAP)\n",
    "#   or: coerce_the_gsheets(gdf, **GEOPANDAS_CONFIG)  # for GeoDataFrames\n",
    "\n",
    "\n",
    "def coerce_the_gsheets(df, dtype_map=None, int_cols=None, float_cols=None, datetime_cols=None, bool_cols=None, category_cols=None, geometry_cols=None, geometry_format='wkt'):\n",
    "    \"\"\"Coerce specified columns on a cleaned DataFrame using coercion helpers.\n",
    "    `dtype_map` is an alternative mapping where keys are 'int','float','datetime','bool','category','geometry'.\n",
    "    `geometry_format` controls geometry coercion: 'wkt' (parse WKT strings) or 'geodataframe' (skip, already typed).\"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        logger.error('coerce_the_gsheets: input is not a DataFrame')\n",
    "        return df\n",
    "    return coerce_columns(df, int_cols=int_cols, float_cols=float_cols, datetime_cols=datetime_cols, bool_cols=bool_cols, category_cols=category_cols, geometry_cols=geometry_cols, dtype_map=dtype_map, geometry_format=geometry_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_the_gsheets(gdf).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export attributes to CSV for downstream processing (optional)\n",
    "output_csv = 'data/shapefile_attributes.csv'\n",
    "gdf.drop(columns='geometry').to_csv(output_csv, index=False)\n",
    "print(f'Attributes exported to {output_csv}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
