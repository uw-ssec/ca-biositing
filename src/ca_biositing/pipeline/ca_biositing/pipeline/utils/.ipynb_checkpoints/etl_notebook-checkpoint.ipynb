{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Notebook for CA Biositing Project\n",
    "\n",
    "This notebook provides a documented walkthrough of the ETL (Extract, Transform, Load) process for the CA Biositing project. It is designed for interactive development and exploration before migrating logic into the production pipeline.\n",
    "\n",
    "It covers:\n",
    "\n",
    "1.  **Setup**: Importing necessary libraries and establishing a connection to the database.\n",
    "2.  **Extraction**: Pulling raw data from Google Sheets.\n",
    "3.  **Cleaning**: Standardizing data types, handling missing values, and cleaning column names.\n",
    "4.  **Normalization**: Replacing human-readable names (e.g., \"Corn\") with database foreign key IDs (e.g., `resource_id: 1`).\n",
    "5.  **Utilities**: Common functions for data manipulation and analysis.\n",
    "6.  **Deployment Plan**: A step-by-step guide for moving the code from this notebook into the production ETL modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor as jn\n",
    "import logging\n",
    "from IPython.display import display\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import select\n",
    "\n",
    "# --- Basic Logging Configuration for Notebook ---\n",
    "# When running in a notebook, we use Python's standard logging.\n",
    "# In the production pipeline, this will be replaced by Prefect's `get_run_logger()`\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- Robustly find the project root ---\n",
    "# This ensures that the notebook can be run from any directory within the project.\n",
    "path = os.getcwd()\n",
    "project_root = None\n",
    "while path != os.path.dirname(path):\n",
    "    if 'pixi.toml' in os.listdir(path):\n",
    "        project_root = path\n",
    "        break\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "if not project_root:\n",
    "    raise FileNotFoundError(\"Could not find project root containing 'pixi.toml'.\")\n",
    "\n",
    "# Add the project root to the Python path to allow for module imports\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    logger.info(f\"Added project root '{project_root}' to sys.path\")\n",
    "else:\n",
    "    logger.info(f\"Project root '{project_root}' is already in sys.path\")\n",
    "\n",
    "# --- Import project modules ---\n",
    "try:\n",
    "    from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.engine import engine\n",
    "    from src.ca_biositing.datamodels.ca_biositing.datamodels.schemas.generated.ca_biositing import *\n",
    "    from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.name_id_swap import replace_name_with_id_df\n",
    "    from src.ca_biositing.pipeline.ca_biositing.pipeline.etl.extract import proximate, ultimate, cmpana\n",
    "    logger.info('Successfully imported all project modules.')\n",
    "except ImportError as e:\n",
    "    logger.error(f'Failed to import project modules: {e}', exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_gsheets(df):\n",
    "    \"\"\"Cleans and standardizes a DataFrame extracted from Google Sheets.\n",
    "\n",
    "    This function performs several key operations:\n",
    "    1. Cleans column names to a standard format (snake_case).\n",
    "    2. Drops rows where essential columns ('repl_no', 'value') are empty.\n",
    "    3. Coerces data types for numeric and datetime columns, handling errors gracefully.\n",
    "    4. Converts remaining columns to the best possible data types.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    logger.info('Starting DataFrame cleaning process.')\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        logger.error('Input is not a pandas DataFrame.')\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 1. Clean names and drop rows with missing essential data\n",
    "        df_cleaned = df.clean_names().dropna(subset=['repl_no', 'value'])\n",
    "        logger.info(f'Dropped {len(df) - len(df_cleaned)} rows with missing values.')\n",
    "\n",
    "        # 2. Coerce numeric types\n",
    "        df_cleaned['repl_no'] = pd.to_numeric(df_cleaned['repl_no'], errors='coerce').astype('Int32')\n",
    "        df_cleaned['value'] = pd.to_numeric(df_cleaned['value'], errors='coerce').astype(np.float32)\n",
    "\n",
    "        # 3. Coerce datetime types\n",
    "        if 'created_at' in df_cleaned.columns:\n",
    "            df_cleaned['created_at'] = pd.to_datetime(df_cleaned['created_at'], errors='coerce')\n",
    "        if 'updated_at' in df_cleaned.columns:\n",
    "            df_cleaned['updated_at'] = pd.to_datetime(df_cleaned['updated_at'], errors='coerce')\n",
    "\n",
    "        # 4. Replace empty strings with NaN so they are properly ignored\n",
    "        df_cleaned = df_cleaned.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "        # 5. Convert other dtypes to best possible\n",
    "        df_cleaned = df_cleaned.convert_dtypes()\n",
    "        logger.info('Successfully cleaned DataFrame.')\n",
    "\n",
    "        # 6. Convert all string data to lowercase\n",
    "        df_cleaned = df_cleaned.applymap(lambda s: s.lower() if isinstance(s, str) else s)\n",
    "        logger.info('Converted all string data to lowercase.')\n",
    "        return df_cleaned\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f'An error occurred during DataFrame cleaning: {e}', exc_info=True)\n",
    "        return None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframes(dataframes, normalize_columns):\n",
    "    \"\"\"Normalizes a list of DataFrames by replacing name columns with foreign key IDs.\n",
    "\n",
    "    This function iterates through a list of dataframes and, for each one, iterates\n",
    "    through a dictionary of columns that need to be normalized. It uses the \n",
    "    `replace_name_with_id_df` utility to look up or create the corresponding ID\n",
    "    in the database.\n",
    "\n",
    "    Args:\n",
    "        dataframes (list[pd.DataFrame]): A list of DataFrames to normalize.\n",
    "        normalize_columns (dict): A dictionary mapping column names to SQLModel classes and attributes.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: The list of normalized DataFrames.\n",
    "    \"\"\"\n",
    "    logger.info(f'Starting normalization process for {len(dataframes)} dataframes.')\n",
    "    normalized_dfs = []\n",
    "    try:\n",
    "        with Session(engine) as db:\n",
    "            for i, df in enumerate(dataframes):\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    logger.warning(f'Item {i+1} is not a DataFrame, skipping.')\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f'Processing DataFrame #{i+1} with {len(df)} rows.')\n",
    "                df_normalized = df.copy()\n",
    "\n",
    "                for df_col, (model, model_name_attr) in normalize_columns.items():\n",
    "                    if df_col not in df_normalized.columns:\n",
    "                        logger.warning(f\"Column '{df_col}' not in DataFrame #{i+1}. Skipping normalization for this column.\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Skip normalization if the column is all NaN/None\n",
    "                        if df_normalized[df_col].isnull().all():\n",
    "                            logger.info(f\"Skipping normalization for column '{df_col}' as it contains only null values.\")\n",
    "                            continue\n",
    "                            \n",
    "                        logger.info(f\"Normalizing column '{df_col}' using model '{model.__name__}'.\")\n",
    "                        df_normalized, num_created = replace_name_with_id_df(\n",
    "                            db=db,\n",
    "                            df=df_normalized,\n",
    "                            ref_model=model,\n",
    "                            df_name_column=df_col,\n",
    "                            model_name_attr=model_name_attr,\n",
    "                            id_column_name='id',\n",
    "                            final_column_name=f'{df_col}_id'\n",
    "                        )\n",
    "                        if num_created > 0:\n",
    "                            logger.info(f\"Created {num_created} new records in '{model.__name__}' table.\")\n",
    "                        new_col_name = f'{df_col}_id'\n",
    "                        num_nulls = df_normalized[new_col_name].isnull().sum()\n",
    "                        logger.info(f\"Successfully normalized '{df_col}'. New column '{new_col_name}' contains {num_nulls} null values.\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error normalizing column '{df_col}' in DataFrame #{i+1}: {e}\", exc_info=True)\n",
    "                        continue # Continue to the next column\n",
    "                \n",
    "                normalized_dfs.append(df_normalized)\n",
    "                logger.info(f'Finished processing DataFrame #{i+1}.')\n",
    "            \n",
    "            logger.info('Committing database session.')\n",
    "            db.commit()\n",
    "            logger.info('Database commit successful.')\n",
    "    except Exception as e:\n",
    "        logger.error(f'A critical error occurred during the database session: {e}', exc_info=True)\n",
    "        db.rollback()\n",
    "        logger.info('Database session rolled back.')\n",
    "        \n",
    "    return normalized_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Execution Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Extraction ---\n",
    "# In a real Prefect flow, each extraction would be a separate task.\n",
    "logger.info('Starting data extraction...')\n",
    "prox_df = proximate.extract(project_root=project_root)\n",
    "ult_df = ultimate.extract(project_root=project_root)\n",
    "cmp_df = cmpana.extract(project_root=project_root)\n",
    "dataframes = [prox_df, ult_df, cmp_df]\n",
    "logger.info('Data extraction complete.')\n",
    "\n",
    "# --- 2. Cleaning ---\n",
    "# This list comprehension applies the cleaning function to each extracted dataframe.\n",
    "logger.info('Starting data cleaning...')\n",
    "clean_dataframes = [clean_the_gsheets(df) for df in dataframes if df is not None]\n",
    "logger.info('Data cleaning complete.')\n",
    "\n",
    "# --- 3. Normalization ---\n",
    "# This dictionary defines the columns to be normalized. \n",
    "# The key is the column name in the DataFrame.\n",
    "# The value is a tuple containing the corresponding SQLAlchemy model and the name of the attribute on the model to match against.\n",
    "NORMALIZE_COLUMNS = {\n",
    "    'resource': (Resource, 'name'),\n",
    "    'prepared_sample': (PreparedSample, 'name'),\n",
    "    'preparation_method': (PreparationMethod, 'name'),\n",
    "    'parameter': (Parameter, 'name'),\n",
    "    'unit': (Unit, 'name'),\n",
    "    'analyst_email': (Contact, 'email'),\n",
    "    'analysis_type': (AnalysisType, 'name'),\n",
    "    'primary_ag_product': (PrimaryAgProduct, 'name')\n",
    "}\n",
    "\n",
    "logger.info('Starting data normalization...')\n",
    "normalized_dataframes = normalize_dataframes(clean_dataframes, NORMALIZE_COLUMNS)\n",
    "logger.info('Data normalization complete.')\n",
    "\n",
    "# --- 4. Display Results ---\n",
    "logger.info('Displaying results of normalization...')\n",
    "for i, df in enumerate(normalized_dataframes):\n",
    "    print(f'--- Normalized DataFrame {i+1} ---')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Plan\n",
    "\n",
    "The code in this notebook will be transitioned to the main ETL pipeline by following these steps:\n",
    "\n",
    "1.  **Function Migration**: The `clean_the_gsheets` and `normalize_dataframes` functions will be moved to a new utility module, for example, `src/ca_biositing/pipeline/ca_biositing/pipeline/utils/etl_utils.py`. Each function will be decorated with `@task` from Prefect to turn it into a reusable pipeline component.\n",
    "2.  **Flow Creation**: A new Prefect flow will be created in the `src/ca_biositing/pipeline/ca_biositing/pipeline/flows/` directory (e.g., `master_extraction_flow.py`). This flow will orchestrate the entire ETL process for a given data source.\n",
    "3.  **Task Integration**: The new flow will be composed of individual tasks. It will call the existing extraction tasks (`proximate.extract`, etc.), and then pass the results to the new cleaning and normalization tasks from `etl_utils.py`.\n",
    "4.  **Logging**: The `logging` module will be replaced with `get_run_logger()` from Prefect within the tasks to ensure logs are captured by the Prefect UI.\n",
    "5.  **Configuration**: The `NORMALIZE_COLUMNS` dictionary will be moved to a configuration file or defined within the relevant flow to make it easier to manage and modify without changing the code.\n",
    "6.  **Testing**: Unit tests will be written for the new utility functions in `etl_utils.py`. An integration test will be created for the new Prefect flow to ensure all the tasks work together correctly.\n",
    "7.  **Deployment**: Once the flow is complete and tested, it will be deployed to the Prefect server using the `pixi run deploy` command, making it available to be run on a schedule or manually via the UI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
