{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Notebook for CA Biositing Project\n",
    "\n",
    "This notebook provides a documented walkthrough of the ETL (Extract, Transform, Load) process for the CA Biositing project. It is designed for interactive development and exploration before migrating logic into the production pipeline.\n",
    "\n",
    "It covers:\n",
    "\n",
    "1.  **Setup**: Importing necessary libraries and establishing a connection to the database.\n",
    "2.  **Extraction**: Pulling raw data from Google Sheets.\n",
    "3.  **Cleaning**: Standardizing data types, handling missing values, and cleaning column names.\n",
    "4.  **Normalization**: Replacing human-readable names (e.g., \"Corn\") with database foreign key IDs (e.g., `resource_id: 1`).\n",
    "5.  **Utilities**: Common functions for data manipulation and analysis.\n",
    "6.  **Deployment Plan**: A step-by-step guide for moving the code from this notebook into the production ETL modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 16:10:18,800 - INFO - Project root 'C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing' is already in sys.path\n",
      "2026-01-13 16:10:18,802 - INFO - Successfully imported all project modules.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor as jn\n",
    "import logging\n",
    "from IPython.display import display\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import select\n",
    "\n",
    "# --- Basic Logging Configuration for Notebook ---\n",
    "# When running in a notebook, we use Python's standard logging.\n",
    "# In the production pipeline, this will be replaced by Prefect's `get_run_logger()`\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- Robustly find the project root ---\n",
    "# This ensures that the notebook can be run from any directory within the project.\n",
    "path = os.getcwd()\n",
    "project_root = None\n",
    "while path != os.path.dirname(path):\n",
    "    if 'pixi.toml' in os.listdir(path):\n",
    "        project_root = path\n",
    "        break\n",
    "    path = os.path.dirname(path)\n",
    "\n",
    "if not project_root:\n",
    "    raise FileNotFoundError(\"Could not find project root containing 'pixi.toml'.\")\n",
    "\n",
    "# Add the project root to the Python path to allow for module imports\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    logger.info(f\"Added project root '{project_root}' to sys.path\")\n",
    "else:\n",
    "    logger.info(f\"Project root '{project_root}' is already in sys.path\")\n",
    "\n",
    "# --- Import project modules ---\n",
    "try:\n",
    "    from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.engine import engine\n",
    "    from src.ca_biositing.datamodels.ca_biositing.datamodels.schemas.generated.ca_biositing import *\n",
    "    from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.name_id_swap import replace_name_with_id_df\n",
    "    from src.ca_biositing.pipeline.ca_biositing.pipeline.etl.extract import biodiesel_plants\n",
    "    logger.info('Successfully imported all project modules.')\n",
    "except ImportError as e:\n",
    "    logger.error(f'Failed to import project modules: {e}', exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_gsheets(df):\n",
    "    \"\"\"Cleans and standardizes a DataFrame extracted from Google Sheets.\n",
    "\n",
    "    This function performs several key operations:\n",
    "    1. Cleans column names to a standard format (snake_case).\n",
    "    2. Drops rows where essential columns ('repl_no', 'value') are empty.\n",
    "    3. Coerces data types for numeric and datetime columns, handling errors gracefully.\n",
    "    4. Converts remaining columns to the best possible data types.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    logger.info('Starting DataFrame cleaning process.')\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        logger.error('Input is not a pandas DataFrame.')\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 1. Clean names and drop rows with missing essential data\n",
    "        df_cleaned = df.clean_names().dropna(subset=['repl_no', 'value'])\n",
    "        logger.info(f'Dropped {len(df) - len(df_cleaned)} rows with missing values.')\n",
    "\n",
    "        # 2. Coerce numeric types\n",
    "        df_cleaned['repl_no'] = pd.to_numeric(df_cleaned['repl_no'], errors='coerce').astype('Int32')\n",
    "        df_cleaned['value'] = pd.to_numeric(df_cleaned['value'], errors='coerce').astype(np.float32)\n",
    "\n",
    "        # 3. Coerce datetime types\n",
    "        if 'created_at' in df_cleaned.columns:\n",
    "            df_cleaned['created_at'] = pd.to_datetime(df_cleaned['created_at'], errors='coerce')\n",
    "        if 'updated_at' in df_cleaned.columns:\n",
    "            df_cleaned['updated_at'] = pd.to_datetime(df_cleaned['updated_at'], errors='coerce')\n",
    "\n",
    "        # 4. Replace empty strings with NaN so they are properly ignored\n",
    "        df_cleaned = df_cleaned.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "        # 5. Convert other dtypes to best possible\n",
    "        df_cleaned = df_cleaned.convert_dtypes()\n",
    "        logger.info('Successfully cleaned DataFrame.')\n",
    "\n",
    "        # 6. Convert all string data to lowercase\n",
    "        df_cleaned = df_cleaned.applymap(lambda s: s.lower() if isinstance(s, str) else s)\n",
    "        logger.info('Converted all string data to lowercase.')\n",
    "        return df_cleaned\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f'An error occurred during DataFrame cleaning: {e}', exc_info=True)\n",
    "        return None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframes(dataframes, normalize_columns):\n",
    "    \"\"\"Normalizes a list of DataFrames by replacing name columns with foreign key IDs.\n",
    "\n",
    "    This function iterates through a list of dataframes and, for each one, iterates\n",
    "    through a dictionary of columns that need to be normalized. It uses the \n",
    "    `replace_name_with_id_df` utility to look up or create the corresponding ID\n",
    "    in the database.\n",
    "\n",
    "    Args:\n",
    "        dataframes (list[pd.DataFrame]): A list of DataFrames to normalize.\n",
    "        normalize_columns (dict): A dictionary mapping column names to SQLModel classes and attributes.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: The list of normalized DataFrames.\n",
    "    \"\"\"\n",
    "    logger.info(f'Starting normalization process for {len(dataframes)} dataframes.')\n",
    "    normalized_dfs = []\n",
    "    try:\n",
    "        with Session(engine) as db:\n",
    "            for i, df in enumerate(dataframes):\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    logger.warning(f'Item {i+1} is not a DataFrame, skipping.')\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f'Processing DataFrame #{i+1} with {len(df)} rows.')\n",
    "                df_normalized = df.copy()\n",
    "\n",
    "                for df_col, (model, model_name_attr) in normalize_columns.items():\n",
    "                    if df_col not in df_normalized.columns:\n",
    "                        logger.warning(f\"Column '{df_col}' not in DataFrame #{i+1}. Skipping normalization for this column.\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Skip normalization if the column is all NaN/None\n",
    "                        if df_normalized[df_col].isnull().all():\n",
    "                            logger.info(f\"Skipping normalization for column '{df_col}' as it contains only null values.\")\n",
    "                            continue\n",
    "                            \n",
    "                        logger.info(f\"Normalizing column '{df_col}' using model '{model.__name__}'.\")\n",
    "                        df_normalized, num_created = replace_name_with_id_df(\n",
    "                            db=db,\n",
    "                            df=df_normalized,\n",
    "                            ref_model=model,\n",
    "                            df_name_column=df_col,\n",
    "                            model_name_attr=model_name_attr,\n",
    "                            id_column_name='id',\n",
    "                            final_column_name=f'{df_col}_id'\n",
    "                        )\n",
    "                        if num_created > 0:\n",
    "                            logger.info(f\"Created {num_created} new records in '{model.__name__}' table.\")\n",
    "                        new_col_name = f'{df_col}_id'\n",
    "                        num_nulls = df_normalized[new_col_name].isnull().sum()\n",
    "                        logger.info(f\"Successfully normalized '{df_col}'. New column '{new_col_name}' contains {num_nulls} null values.\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error normalizing column '{df_col}' in DataFrame #{i+1}: {e}\", exc_info=True)\n",
    "                        continue # Continue to the next column\n",
    "                \n",
    "                normalized_dfs.append(df_normalized)\n",
    "                logger.info(f'Finished processing DataFrame #{i+1}.')\n",
    "            \n",
    "            logger.info('Committing database session.')\n",
    "            db.commit()\n",
    "            logger.info('Database commit successful.')\n",
    "    except Exception as e:\n",
    "        logger.error(f'A critical error occurred during the database session: {e}', exc_info=True)\n",
    "        db.rollback()\n",
    "        logger.info('Database session rolled back.')\n",
    "        \n",
    "    return normalized_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Execution Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydrive2.auth import GoogleAuth, AuthenticationError\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from pydrive2.files import ApiRequestError\n",
    "\n",
    "def gdrive_to_df(file_name: str, mime_type: str, credentials_path: str, dataset_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts data from a CSV, ZIP, or GEOJSON file into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_name: The name of the requested file.\n",
    "        mime_type: The MIME type - according to https://mime-type.com/\n",
    "        credentials_path: The path to the Google Cloud service account credentials JSON file.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the data from the specified worksheet, or None on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        settings = {\n",
    "                \"client_config_backend\": \"service\",\n",
    "                \"service_config\": {\n",
    "                    \"client_json_file_path\": credentials_path,\n",
    "                }\n",
    "            }\n",
    "        # Create instance of GoogleAuth\n",
    "        gauth = GoogleAuth(settings=settings)\n",
    "        gauth.ServiceAuth()\n",
    "        drive = GoogleDrive(gauth)\n",
    "\n",
    "        try:\n",
    "            file_entries = drive.ListFile({\"q\": f\"title = '{file_name}' and mimeType= '{mime_type}'\"}).GetList()\n",
    "            if len(file_entries) == 0: \n",
    "                raise FileNotFoundError(f\"Error: File '{file_name}' not found. \\n Please make sure the spreadsheet name is correct and that you have shared it with the service account email.\")\n",
    "                return None\n",
    "            else:\n",
    "                file_entry = file_entries[0]\n",
    "            file = drive.CreateFile({'id': file_entry['id']})\n",
    "            file.GetContentFile(dataset_folder + file_name) # Download file\n",
    "        except ApiRequestError:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Use the first row as header and the rest as data\n",
    "        if mime_type == \"text/csv\":\n",
    "            df = pd.read_csv(dataset_folder + file_name)\n",
    "\n",
    "        # De-duplicate columns, keeping the first occurrence\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "        return df\n",
    "\n",
    "    except AuthenticationError as e:\n",
    "        print(f\"Google Authentication Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import pandas as pd\n",
    "from prefect import task, get_run_logger\n",
    "# from src.ca_biositing.pipeline.ca_biositing.pipeline.utils.gdrive_to_pandas import gdrive_to_df\n",
    "# from resources.prefect import credentials\n",
    "\n",
    "@task\n",
    "def extract(project_root: Optional[str] = None) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Extracts raw data from a .csv file.\n",
    "\n",
    "    This function serves as the 'Extract' step in an ETL pipeline. It connects\n",
    "    to the data source and returns the data as is, without transformation.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the raw data, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    FILE_NAME = \"Biodiesel_Plants.csv\"\n",
    "    MIME_TYPE = \"text/csv\"\n",
    "    CREDENTIALS_PATH = \"credentials.json\"\n",
    "    DATASET_FOLDER = \"src/ca_biositing/pipeline/ca_biositing/pipeline/temp_external_datasets/\" \n",
    "    logger.info(f\"Extracting raw data from '{FILE_NAME}'...\")\n",
    "\n",
    "    # If project_root is provided (e.g., from a notebook), construct an absolute path\n",
    "    # Otherwise, use the default relative path (for the main pipeline)\n",
    "    credentials_path = CREDENTIALS_PATH\n",
    "    dataset_folder = DATASET_FOLDER\n",
    "    if project_root:\n",
    "        credentials_path = os.path.join(project_root, CREDENTIALS_PATH)\n",
    "        dataset_folder = os.path.join(project_root, DATASET_FOLDER)\n",
    "\n",
    "    # The gsheet_to_df function handles authentication, data fetching, and error handling.\n",
    "    raw_df = gdrive_to_df(FILE_NAME, MIME_TYPE, credentials_path, dataset_folder)\n",
    "\n",
    "    \n",
    "\n",
    "    if raw_df is None:\n",
    "        logger.error(\"Failed to extract data. Aborting.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(\"Successfully extracted raw data.\")\n",
    "    return raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 18:36:20,831 - INFO - Starting data extraction...\n",
      "2026-01-13 18:36:20,856 - INFO - HTTP Request: GET http://localhost:4200/api/admin/version \"HTTP/1.1 200 OK\"\n",
      "2026-01-13 18:36:20,857 - WARNING - Your Prefect server is running an older version of Prefect than your client which may result in unexpected behavior. Please upgrade your Prefect server from version 3.5.0 to version 3.6.7 or higher.\n",
      "2026-01-13 18:36:20,866 - INFO - Extracting raw data from 'Biodiesel_Plants.csv'...\n",
      "2026-01-13 18:36:20,886 - INFO - Attempting refresh to obtain initial access_token\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\_openssl_crypt.py:97: DeprecationWarning: sign() is deprecated. Use the equivalent APIs in cryptography.\n",
      "  return crypto.sign(self._key, message, 'sha256')\n",
      "2026-01-13 18:36:20,889 - INFO - Refreshing access_token\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\client.py:789: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  self.token_expiry = delta + _UTCNOW()\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\client.py:647: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = _UTCNOW()\n",
      "2026-01-13 18:36:22,092 - INFO - Successfully extracted raw data.\n",
      "2026-01-13 18:36:22,097 - INFO - Finished in state Completed()\n",
      "2026-01-13 18:36:22,113 - INFO - Data extraction complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                  company  bbi_index             city  \\\n",
      "0                     American GreenFuels        NaN        New Haven   \n",
      "1                Down To Earth Energy LLC        NaN           Monroe   \n",
      "2                      Maine Bio-Fuel Inc        NaN         Portland   \n",
      "3                   Cape Cod Biofuels Inc        NaN         Sandwich   \n",
      "4             Renewable Fuels by Peterson        NaN  North Haverhill   \n",
      "..                                    ...        ...              ...   \n",
      "73                     Walsh BioFuels LLC       58.0          Mauston   \n",
      "74  Western Iowa Energy - Agron Bioenergy       60.0      Watsonville   \n",
      "75           White Mountain Biodiesel LLC       62.0  North Haverhill   \n",
      "76                 World Energy - Natchez       65.0          Natchez   \n",
      "77                    World Energy - Rome       66.0             Rome   \n",
      "\n",
      "            state  capacity_mmg_per_y            feedstock       status  \\\n",
      "0     Connecticut                  35                  NaN          NaN   \n",
      "1         Georgia                   2                  NaN          NaN   \n",
      "2           Maine                   1                  NaN          NaN   \n",
      "3   Massachusetts                   1                  NaN          NaN   \n",
      "4   New Hampshire                   8                  NaN          NaN   \n",
      "..            ...                 ...                  ...          ...   \n",
      "73             WI                   5  Distillers corn oil  Operational   \n",
      "74             CA                  15       Multifeedstock  Operational   \n",
      "75             NH                   8        Yellow grease  Operational   \n",
      "76             MS                  72       Vegetable oils  Operational   \n",
      "77             GA                  20       Multifeedstock  Operational   \n",
      "\n",
      "                                         address  \\\n",
      "0                                            NaN   \n",
      "1                                            NaN   \n",
      "2                                            NaN   \n",
      "3                                            NaN   \n",
      "4                                            NaN   \n",
      "..                                           ...   \n",
      "73         WI-80 & Mauston Rd, Mauston, WI 53948   \n",
      "74         860 W Beach St, Watsonville, CA 95076   \n",
      "75  35 Business Pk Rd, North Haverhill, NH 03774   \n",
      "76               L E Barry Rd, Natchez, MS 39120   \n",
      "77         555 W Hermitage Rd NE, Rome, GA 30161   \n",
      "\n",
      "                               coordinates   latitude   longitude  \\\n",
      "0                                      NaN  41.290100  -72.902900   \n",
      "1                                      NaN  33.757170  -83.727700   \n",
      "2                                      NaN  43.691400  -70.328100   \n",
      "3                                      NaN  41.717700  -70.484500   \n",
      "4                                      NaN  44.077000  -72.004700   \n",
      "..                                     ...        ...         ...   \n",
      "73  43.777703209506015, -90.05515309603231  43.777703  -90.055153   \n",
      "74  36.90388067222262, -121.76968983169887  36.903881 -121.769690   \n",
      "75    44.07671166339629, -72.0049135970569  44.076712  -72.004914   \n",
      "76   31.533227585047626, -91.4374585606938  31.533228  -91.437459   \n",
      "77   34.327173247943826, -85.0919307299404  34.327173  -85.091931   \n",
      "\n",
      "                                               source  \n",
      "0   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "1   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "2   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "3   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "4   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "..                                                ...  \n",
      "73  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "74  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "75  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "76  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "77  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "\n",
      "[78 rows x 12 columns]]\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Extraction ---\n",
    "# In a real Prefect flow, each extraction would be a separate task.\n",
    "logger.info('Starting data extraction...')\n",
    "biodiesel_plants_df = extract(project_root=project_root)\n",
    "dataframes = [biodiesel_plants_df]\n",
    "print(dataframes)\n",
    "logger.info('Data extraction complete.')\n",
    "\n",
    "# # --- 2. Cleaning ---\n",
    "# # This list comprehension applies the cleaning function to each extracted dataframe.\n",
    "# logger.info('Starting data cleaning...')\n",
    "# clean_dataframes = [clean_the_gsheets(df) for df in dataframes if df is not None]\n",
    "# logger.info('Data cleaning complete.')\n",
    "\n",
    "# # --- 3. Normalization ---\n",
    "# # This dictionary defines the columns to be normalized. \n",
    "# # The key is the column name in the DataFrame.\n",
    "# # The value is a tuple containing the corresponding SQLAlchemy model and the name of the attribute on the model to match against.\n",
    "# NORMALIZE_COLUMNS = {\n",
    "#     'resource': (Resource, 'name'),\n",
    "#     'prepared_sample': (PreparedSample, 'name'),\n",
    "#     'preparation_method': (PreparationMethod, 'name'),\n",
    "#     'parameter': (Parameter, 'name'),\n",
    "#     'unit': (Unit, 'name'),\n",
    "#     'analyst_email': (Contact, 'email'),\n",
    "#     'analysis_type': (AnalysisType, 'name'),\n",
    "#     'primary_ag_product': (PrimaryAgProduct, 'name')\n",
    "# }\n",
    "\n",
    "# logger.info('Starting data normalization...')\n",
    "# normalized_dataframes = normalize_dataframes(clean_dataframes, NORMALIZE_COLUMNS)\n",
    "# logger.info('Data normalization complete.')\n",
    "\n",
    "# # --- 4. Display Results ---\n",
    "# logger.info('Displaying results of normalization...')\n",
    "# for i, df in enumerate(normalized_dataframes):\n",
    "#     print(f'--- Normalized DataFrame {i+1} ---')\n",
    "#     display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Plan\n",
    "\n",
    "The code in this notebook will be transitioned to the main ETL pipeline by following these steps:\n",
    "\n",
    "1.  **Function Migration**: The `clean_the_gsheets` and `normalize_dataframes` functions will be moved to a new utility module, for example, `src/ca_biositing/pipeline/ca_biositing/pipeline/utils/etl_utils.py`. Each function will be decorated with `@task` from Prefect to turn it into a reusable pipeline component.\n",
    "2.  **Flow Creation**: A new Prefect flow will be created in the `src/ca_biositing/pipeline/ca_biositing/pipeline/flows/` directory (e.g., `master_extraction_flow.py`). This flow will orchestrate the entire ETL process for a given data source.\n",
    "3.  **Task Integration**: The new flow will be composed of individual tasks. It will call the existing extraction tasks (`proximate.extract`, etc.), and then pass the results to the new cleaning and normalization tasks from `etl_utils.py`.\n",
    "4.  **Logging**: The `logging` module will be replaced with `get_run_logger()` from Prefect within the tasks to ensure logs are captured by the Prefect UI.\n",
    "5.  **Configuration**: The `NORMALIZE_COLUMNS` dictionary will be moved to a configuration file or defined within the relevant flow to make it easier to manage and modify without changing the code.\n",
    "6.  **Testing**: Unit tests will be written for the new utility functions in `etl_utils.py`. An integration test will be created for the new Prefect flow to ensure all the tasks work together correctly.\n",
    "7.  **Deployment**: Once the flow is complete and tested, it will be deployed to the Prefect server using the `pixi run deploy` command, making it available to be run on a schedule or manually via the UI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
