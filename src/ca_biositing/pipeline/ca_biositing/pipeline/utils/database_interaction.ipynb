{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Database Querying\n",
    "\n",
    "This notebook demonstrates how to connect to the project's PostgreSQL database from a local Jupyter environment to perform interactive queries and data analysis. It is intended as an educational tool and a template for data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Necessary Libraries\n",
    "\n",
    "We begin by importing the required libraries:\n",
    "- `pandas` for data manipulation.\n",
    "- `create_engine`, `Session`, and `select` from `sqlmodel` for database interaction.\n",
    "- The specific SQLModel data models (`FieldSample`, `GeographicLocation`) we want to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and models imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlmodel import create_engine, Session, select\n",
    "\n",
    "try:\n",
    "    from ca_biositing.datamodels.biomass import FieldSample\n",
    "    from ca_biositing.datamodels.biomass import PrimaryProduct\n",
    "    from ca_biositing.datamodels.geographic_locations import GeographicLocation\n",
    "    print(\"Libraries and models imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing models: {e}\")\n",
    "    print(\"Please ensure the project is installed correctly by running 'pixi install' in your terminal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Database Engine\n",
    "\n",
    "Next, we create a SQLAlchemy engine to manage connections to the database. \n",
    "\n",
    "**Note:** We are hardcoding the database URL here to ensure the notebook runs reliably. It connects to `localhost:5432`, which is the default port mapped to the project's Dockerized database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database engine created. Ready to connect to: postgresql+psycopg2://biocirv_user:<password>@localhost:5432/biocirv_db\n"
     ]
    }
   ],
   "source": [
    "DATABASE_URL = \"postgresql+psycopg2://biocirv_user:biocirv_dev_password@localhost:5432/biocirv_db\"\n",
    "\n",
    "try:\n",
    "    engine = create_engine(DATABASE_URL, echo=False)\n",
    "    print(f\"Database engine created. Ready to connect to: {DATABASE_URL.replace('biocirv_dev_password', '<password>')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating database engine: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Query the Tables\n",
    "\n",
    "Now we can use the engine to open a session and query our tables. We will query the `field_samples` and `geographic_locations` tables and load them into separate pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loaded 0 records from 'field_samples'.\n",
      "- Loaded 0 records from 'geographic_locations'.\n",
      "- Loaded 18 records from 'primary_product'.\n"
     ]
    }
   ],
   "source": [
    "field_samples_df = None\n",
    "locations_df = None\n",
    "primary_product_df = None\n",
    "\n",
    "try:\n",
    "    with Session(engine) as session:\n",
    "        # Query the field_samples table\n",
    "        statement_samples = select(FieldSample)\n",
    "        results_samples = session.exec(statement_samples).all()\n",
    "        field_samples_df = pd.DataFrame([sample.model_dump() for sample in results_samples])\n",
    "        print(f\"- Loaded {len(field_samples_df)} records from 'field_samples'.\")\n",
    "        \n",
    "        # Query the geographic_locations table\n",
    "        statement_locations = select(GeographicLocation)\n",
    "        results_locations = session.exec(statement_locations).all()\n",
    "        locations_df = pd.DataFrame([location.model_dump() for location in results_locations])\n",
    "        print(f\"- Loaded {len(locations_df)} records from 'geographic_locations'.\")\n",
    "\n",
    "                # Query the primary_product table\n",
    "        statement_products = select(PrimaryProduct)\n",
    "        results_products = session.exec(statement_products).all()\n",
    "        primary_product_df = pd.DataFrame([product.model_dump() for product in results_products])\n",
    "        print(f\"- Loaded {len(primary_product_df)} records from 'primary_product'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during database query: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "analysis_type_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "analysis_name",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "29cf402a-5519-4cf9-9c5d-27d39dd02736",
       "rows": [
        [
         "1",
         "Proximate analysis"
        ],
        [
         "2",
         "ICP-OES"
        ],
        [
         "3",
         "Chemical composition"
        ],
        [
         "4",
         "XRF analysis"
        ],
        [
         "5",
         "Ultimate analysis"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis_type_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Proximate analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICP-OES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chemical composition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XRF analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ultimate analysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         analysis_name\n",
       "analysis_type_id                      \n",
       "1                   Proximate analysis\n",
       "2                              ICP-OES\n",
       "3                 Chemical composition\n",
       "4                         XRF analysis\n",
       "5                    Ultimate analysis"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_sql(\"SELECT * FROM analysis_types\", engine)\n",
    "df2 = df2.set_index(\"analysis_type_id\")\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Join the DataFrames\n",
    "\n",
    "With our data loaded into DataFrames, we can now perform a standard `left` join using pandas to combine them on the `location_id` key. \n",
    "\n",
    "We include error handling to manage the case where the database is empty. If the DataFrames are empty, the join key might not exist, and the merge will fail. Our code will catch this and print a helpful message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping join because 'field_samples' table is empty.\n"
     ]
    }
   ],
   "source": [
    "merged_df = None\n",
    "\n",
    "try:\n",
    "    # Ensure dataframes are not None and not empty before merging\n",
    "    if field_samples_df is not None and not field_samples_df.empty:\n",
    "        merged_df = pd.merge(\n",
    "            left=field_samples_df, \n",
    "            right=locations_df, \n",
    "            how='left', \n",
    "            on='location_id'\n",
    "        )\n",
    "        print(\"Join complete.\")\n",
    "    else:\n",
    "        print(\"Skipping join because 'field_samples' table is empty.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error during join: Could not find join key {e}. This can happen if the tables are empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during the join: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Display Results\n",
    "\n",
    "Finally, we can display the head of the merged DataFrame to inspect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data to display.\n"
     ]
    }
   ],
   "source": [
    "if merged_df is not None:\n",
    "    display(merged_df.head())\n",
    "else:\n",
    "    print(\"No data to display.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
