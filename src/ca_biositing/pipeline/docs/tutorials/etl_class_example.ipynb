{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad85317",
   "metadata": {},
   "source": [
    "# ETL Class Example Notebook\n",
    "\n",
    "This notebook demonstrates the three main steps of the ETL pipeline: **Extract**, **Transform**, and **Load**. It mirrors the structure of `etl_notebook.ipynb` but provides a concise classâ€‘based example for quick reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19476297",
   "metadata": {},
   "source": [
    "## Extract\n",
    "\n",
    "Set up the project root on `sys.path` so that package imports work from any working directory. Import the extraction utilities required for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb3498d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "from sqlalchemy.orm import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18423174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">13:00:49.964 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Extracting raw data from 'samplemetadata' in 'Sampling_data_redacted'...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "13:00:49.964 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Extracting raw data from 'samplemetadata' in 'Sampling_data_redacted'...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: gsheet_to_df called for Sampling_data_redacted / samplemetadata\n",
      "DEBUG: Authenticating with credentials.json\n",
      "DEBUG: Opening spreadsheet Sampling_data_redacted\n",
      "DEBUG: Opening worksheet samplemetadata\n",
      "DEBUG: Fetching all values from samplemetadata\n",
      "DEBUG: Successfully fetched 106 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">13:00:51.195 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Successfully extracted raw data.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "13:00:51.195 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Successfully extracted raw data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">13:00:51.198 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "13:00:51.198 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">13:00:51.222 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Extracting raw data from 'static_resource_info' in 'Static_resource_information'...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "13:00:51.222 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Extracting raw data from 'static_resource_info' in 'Static_resource_information'...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: gsheet_to_df called for Static_resource_information / static_resource_info\n",
      "DEBUG: Authenticating with credentials.json\n",
      "DEBUG: Opening spreadsheet Static_resource_information\n",
      "DEBUG: Opening worksheet static_resource_info\n",
      "DEBUG: Fetching all values from static_resource_info\n",
      "DEBUG: Successfully fetched 95 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">13:00:52.426 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Successfully extracted raw data.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "13:00:52.426 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Successfully extracted raw data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">13:00:52.430 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'extract' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "13:00:52.430 | \u001b[36mINFO\u001b[0m    | Task run 'extract' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "a8d27f48-c706-4558-8bd9-2c2966b1526c",
       "rows": [
        [
         "Resource",
         "object"
        ],
        [
         "Resource Code",
         "object"
        ],
        [
         "LandIQ Crop Name",
         "object"
        ],
        [
         "Residue Type",
         "object"
        ],
        [
         "Collected?",
         "object"
        ],
        [
         "From Month",
         "object"
        ],
        [
         "To Month",
         "object"
        ],
        [
         "Residue Yield (Wet Ton/Ac)",
         "object"
        ],
        [
         "Moisture Content",
         "object"
        ],
        [
         "Residue Yield (Dry Ton/Ac)",
         "object"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "Resource                      object\n",
       "Resource Code                 object\n",
       "LandIQ Crop Name              object\n",
       "Residue Type                  object\n",
       "Collected?                    object\n",
       "From Month                    object\n",
       "To Month                      object\n",
       "Residue Yield (Wet Ton/Ac)    object\n",
       "Moisture Content              object\n",
       "Residue Yield (Dry Ton/Ac)    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ca_biositing.datamodels.config import settings\n",
    "from ca_biositing.pipeline.etl.extract import proximate, samplemetadata, ultimate, cmpana, static_resource_info \n",
    "\n",
    "# Extract data\n",
    "samplemetadata_raw = samplemetadata.extract()   \n",
    "static_resource_info_raw = static_resource_info.extract()\n",
    "#prox_raw = proximate.extract()\n",
    "#ult_raw = ultimate.extract()\n",
    "#cmpana_raw = cmpana.extract()\n",
    "\n",
    "#feedstock_collector_info_raw = feedstock_collection_info.extract()\n",
    "\n",
    "#sampleids_raw = sample_ids.extract()\n",
    "\n",
    "#analysis_data = [prox_raw, ult_raw, cmpana_raw]\n",
    "\n",
    "static_resource_info_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d09ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "476199bd",
   "metadata": {},
   "source": [
    "## Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2781377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ca_biositing.pipeline.utils.cleaning_functions import cleaning as cleaning_mod\n",
    "\n",
    "cleaned_data = []\n",
    "for df in analysis_data:\n",
    "    df['dataset'] = 'biocirv'\n",
    "    cleaned_df = cleaning_mod.standard_clean(df)\n",
    "    cleaned_data.append(cleaned_df)\n",
    "\n",
    "print(f\"Cleaned {len(cleaned_data)} dataframes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ad4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ca_biositing.pipeline.utils.cleaning_functions import coercion as coercion_mod\n",
    "\n",
    "coerced_data = []\n",
    "for df in cleaned_data:\n",
    "    # Example: coerce columns into the designated data types (int, float, datetime, geom, etc)\n",
    "    coerced_df = coercion_mod.coerce_columns(df,\n",
    "                                             int_cols=['repl_no'], \n",
    "                                             float_cols=['value'],\n",
    "                                             datetime_cols=['created_at', 'updated_at'])\n",
    "    coerced_data.append(coerced_df)\n",
    "\n",
    "print(f\"Coerced {len(coerced_data)} dataframes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ca_biositing.pipeline.utils.name_id_swap import normalize_dataframes\n",
    "from ca_biositing.datamodels.schemas.generated.ca_biositing import *\n",
    "\n",
    "normalize_columns = {\n",
    "    'resource': (Resource, 'name'),\n",
    "    'prepared_sample': (PreparedSample, 'name'),\n",
    "    'preparation_method': (Method, 'name'),\n",
    "    'parameter': (Parameter, 'name'),\n",
    "    'unit': (Unit, 'name'),\n",
    "    'sample_unit': (Unit, 'name'),\n",
    "    'analyst_email': (Contact, 'email'),\n",
    "    'primary_ag_product': (PrimaryAgProduct, 'name'),\n",
    "    'provider_code': (Provider, 'codename'),\n",
    "    'dataset': (Dataset, 'name')\n",
    "}\n",
    "\n",
    "normalized_data = []\n",
    "for df in coerced_data:\n",
    "    normalized_df = normalize_dataframes(df, normalize_columns)\n",
    "    normalized_data.append(normalized_df)\n",
    "\n",
    "print(f\"Normalized {len(normalized_data)} dataframes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2def234",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Record Information DataFrames\n",
    "\n",
    "observation_data = []\n",
    "for df in normalized_data:\n",
    "    obs_df = df[[\n",
    "        'dataset_id',\n",
    "        'analysis_type', \n",
    "        'record_id',\n",
    "        'parameter_id',\n",
    "        'value',\n",
    "        'unit_id', \n",
    "        'note'\n",
    "    ]].copy().rename(columns={'analysis_type': 'record_type'})\n",
    "    obs_df = obs_df.dropna(subset=['record_id', 'parameter_id', 'value'])\n",
    "    observation_data.append(obs_df)\n",
    "\n",
    "print(f\"Prepared {len(observation_data)} observation dataframes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Record Information DataFrames\n",
    "\n",
    "record_data = []\n",
    "for df in normalized_data:\n",
    "    # 1. Define explicit mappings for non-normalized columns\n",
    "    rename_map = {\n",
    "        'record_id': 'record_id',\n",
    "        'repl_no': 'technical_replication_no',\n",
    "        'qc_result': 'qc_pass',\n",
    "        'note': 'note'\n",
    "    }\n",
    "    \n",
    "    # 2. Dynamically add normalized columns from the normalize_columns dictionary\n",
    "    for col in normalize_columns.keys():\n",
    "        norm_col = f\"{col}_id\"\n",
    "        if norm_col in df.columns:\n",
    "            # Special case: rename to match target record table schema\n",
    "            target_name = 'analyst_id' if col == 'analyst_email' else \\\n",
    "                          'method_id' if col == 'preparation_method' else norm_col\n",
    "            rename_map[norm_col] = target_name\n",
    "\n",
    "    # 3. Only select columns that actually exist in this specific dataframe\n",
    "    available_cols = [c for c in rename_map.keys() if c in df.columns]\n",
    "    final_rename = {k: v for k, v in rename_map.items() if k in available_cols}\n",
    "\n",
    "    record_df = df[available_cols].copy().rename(columns=final_rename)\n",
    "\n",
    "    # 4. Drop rows where critical identifiers are missing (NaN)\n",
    "    if 'record_id' in record_df.columns:\n",
    "        record_df = record_df.dropna(subset=['record_id'])\n",
    "    \n",
    "    record_data.append(record_df)\n",
    "\n",
    "print(f\"Prepared {len(record_data)} record dataframes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d92bf7",
   "metadata": {},
   "source": [
    "## Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = settings.database_url\n",
    "if \"@db:\" in db_url:\n",
    "    db_url = db_url.replace(\"@db:\", \"@localhost:\")\n",
    "elif \"db:5432\" in db_url:\n",
    "    db_url = db_url.replace(\"db:5432\", \"localhost:5432\")\n",
    "\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "def upsert_observations(df, session):\n",
    "    if df.empty:\n",
    "        return\n",
    "    now = datetime.now(timezone.utc)\n",
    "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
    "    for record in records:\n",
    "        record['updated_at'] = now\n",
    "        if record.get('created_at') is None:\n",
    "            record['created_at'] = now\n",
    "        stmt = insert(Observation).values(record)\n",
    "        update_dict = {\n",
    "            c.name: stmt.excluded[c.name]\n",
    "            for c in Observation.__table__.columns\n",
    "            if c.name not in ['id', 'created_at', 'record_id']\n",
    "        }\n",
    "        upsert_stmt = stmt.on_conflict_do_update(\n",
    "            index_elements=['record_id'],\n",
    "            set_=update_dict\n",
    "        )\n",
    "        session.execute(upsert_stmt)\n",
    "\n",
    "with Session(engine) as session:\n",
    "    for obs_df in observation_data:\n",
    "        upsert_observations(obs_df, session)\n",
    "    session.commit()\n",
    "print('Upsert of all observations completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upsert-proximate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ca_biositing.datamodels.schemas.generated.ca_biositing import ProximateRecord\n",
    "\n",
    "def upsert_proximate_records(df, session):\n",
    "    if df.empty:\n",
    "        return\n",
    "    now = datetime.now(timezone.utc)\n",
    "    # Filter record dictionary to only include columns that exist in the table\n",
    "    table_columns = {c.name for c in ProximateRecord.__table__.columns}\n",
    "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
    "    for record in records:\n",
    "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
    "        clean_record['updated_at'] = now\n",
    "        if clean_record.get('created_at') is None:\n",
    "            clean_record['created_at'] = now\n",
    "        stmt = insert(ProximateRecord).values(clean_record)\n",
    "        update_dict = {\n",
    "            c.name: stmt.excluded[c.name]\n",
    "            for c in ProximateRecord.__table__.columns\n",
    "            if c.name not in ['id', 'created_at', 'record_id']\n",
    "        }\n",
    "        upsert_stmt = stmt.on_conflict_do_update(\n",
    "            index_elements=['record_id'],\n",
    "            set_=update_dict\n",
    "        )\n",
    "        session.execute(upsert_stmt)\n",
    "\n",
    "with Session(engine) as session:\n",
    "    # Assuming the first dataframe in record_data is Proximate\n",
    "    upsert_proximate_records(record_data[0], session)\n",
    "    session.commit()\n",
    "print('Upsert of Proximate records completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upsert-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ca_biositing.datamodels.schemas.generated.ca_biositing import UltimateRecord\n",
    "\n",
    "def upsert_ultimate_records(df, session):\n",
    "    if df.empty:\n",
    "        return\n",
    "    now = datetime.now(timezone.utc)\n",
    "    table_columns = {c.name for c in UltimateRecord.__table__.columns}\n",
    "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
    "    for record in records:\n",
    "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
    "        clean_record['updated_at'] = now\n",
    "        if clean_record.get('created_at') is None:\n",
    "            clean_record['created_at'] = now\n",
    "        stmt = insert(UltimateRecord).values(clean_record)\n",
    "        update_dict = {\n",
    "            c.name: stmt.excluded[c.name]\n",
    "            for c in UltimateRecord.__table__.columns\n",
    "            if c.name not in ['id', 'created_at', 'record_id']\n",
    "        }\n",
    "        upsert_stmt = stmt.on_conflict_do_update(\n",
    "            index_elements=['record_id'],\n",
    "            set_=update_dict\n",
    "        )\n",
    "        session.execute(upsert_stmt)\n",
    "\n",
    "with Session(engine) as session:\n",
    "    # Assuming the second dataframe in record_data is Ultimate\n",
    "    upsert_ultimate_records(record_data[1], session)\n",
    "    session.commit()\n",
    "print('Upsert of Ultimate records completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upsert-compositional",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ca_biositing.datamodels.schemas.generated.ca_biositing import CompositionalRecord\n",
    "\n",
    "def upsert_compositional_records(df, session):\n",
    "    if df.empty:\n",
    "        return\n",
    "    now = datetime.now(timezone.utc)\n",
    "    table_columns = {c.name for c in CompositionalRecord.__table__.columns}\n",
    "    records = df.replace({np.nan: None}).to_dict(orient='records')\n",
    "    for record in records:\n",
    "        clean_record = {k: v for k, v in record.items() if k in table_columns}\n",
    "        clean_record['updated_at'] = now\n",
    "        if clean_record.get('created_at') is None:\n",
    "            clean_record['created_at'] = now\n",
    "        stmt = insert(CompositionalRecord).values(clean_record)\n",
    "        update_dict = {\n",
    "            c.name: stmt.excluded[c.name]\n",
    "            for c in CompositionalRecord.__table__.columns\n",
    "            if c.name not in ['id', 'created_at', 'record_id']\n",
    "        }\n",
    "        upsert_stmt = stmt.on_conflict_do_update(\n",
    "            index_elements=['record_id'],\n",
    "            set_=update_dict\n",
    "        )\n",
    "        session.execute(upsert_stmt)\n",
    "\n",
    "with Session(engine) as session:\n",
    "    # Assuming the third dataframe in record_data is Compositional\n",
    "    upsert_compositional_records(record_data[2], session)\n",
    "    session.commit()\n",
    "print('Upsert of Compositional records completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedstock_collector_info_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ca_biositing.pipeline.etl.extract.biodiesel_plants import extract as biodiesel_extract\n",
    "biodiesel_plants_raw = biodiesel_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "biodiesel_plants_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from ca_biositing.pipeline.etl.extract.basic_sample_info import extract as basic_sample_info_extract\n",
    "from ca_biositing.pipeline.etl.transform.resource import transform as resource_transform\n",
    "\n",
    "basic_sample_info = basic_sample_info_extract()\n",
    "\n",
    "EXTRACT_SOURCES: List[str] = [\"basic_sample_info\"]\n",
    "\n",
    "cleaned_data = resource_transform.fn({\"basic_sample_info\": basic_sample_info})\n",
    "\n",
    "cleaned_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
