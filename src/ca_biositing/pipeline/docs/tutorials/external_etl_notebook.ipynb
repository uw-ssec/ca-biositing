{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Notebook for CA Biositing Project\n",
    "\n",
    "This notebook provides a documented walkthrough of the ETL (Extract, Transform, Load) process for the CA Biositing project. It is designed for interactive development and exploration before migrating logic into the production pipeline.\n",
    "\n",
    "It covers:\n",
    "\n",
    "1.  **Setup**: Importing necessary libraries and establishing a connection to the database.\n",
    "2.  **Extraction**: Pulling raw data from Google Sheets.\n",
    "3.  **Cleaning**: Standardizing data types, handling missing values, and cleaning column names.\n",
    "4.  **Normalization**: Replacing human-readable names (e.g., \"Corn\") with database foreign key IDs (e.g., `resource_id: 1`).\n",
    "5.  **Utilities**: Common functions for data manipulation and analysis.\n",
    "6.  **Deployment Plan**: A step-by-step guide for moving the code from this notebook into the production ETL modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport janitor as jn\nimport logging\nfrom IPython.display import display\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import select\n\n# --- Basic Logging Configuration for Notebook ---\n# When running in a notebook, we use Python's standard logging.\n# In the production pipeline, this will be replaced by Prefect's `get_run_logger()`\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger()\n\n# --- Import project modules ---\n# All namespace packages are installed in the Pixi environment.\n# Use the Pixi kernel in Jupyter to ensure correct imports.\ntry:\n    from ca_biositing.pipeline.utils.engine import engine\n    from ca_biositing.datamodels.schemas.generated.ca_biositing import *\n    from ca_biositing.pipeline.utils.name_id_swap import replace_name_with_id_df\n    from ca_biositing.pipeline.etl.extract import biodiesel_plants\n    from ca_biositing.pipeline.etl.extract import ca_proc_points\n    from ca_biositing.pipeline.etl.extract import petroleum_pipelines\n    logger.info('Successfully imported all project modules.')\nexcept ImportError as e:\n    logger.error(f'Failed to import project modules: {e}', exc_info=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_gsheets(df):\n",
    "    \"\"\"Cleans and standardizes a DataFrame extracted from Google Sheets.\n",
    "\n",
    "    This function performs several key operations:\n",
    "    1. Cleans column names to a standard format (snake_case).\n",
    "    2. Drops rows where essential columns ('repl_no', 'value') are empty.\n",
    "    3. Coerces data types for numeric and datetime columns, handling errors gracefully.\n",
    "    4. Converts remaining columns to the best possible data types.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    logger.info('Starting DataFrame cleaning process.')\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        logger.error('Input is not a pandas DataFrame.')\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 1. Clean names and drop rows with missing essential data\n",
    "        df_cleaned = df.clean_names().dropna(subset=['repl_no', 'value'])\n",
    "        logger.info(f'Dropped {len(df) - len(df_cleaned)} rows with missing values.')\n",
    "\n",
    "        # 2. Coerce numeric types\n",
    "        df_cleaned['repl_no'] = pd.to_numeric(df_cleaned['repl_no'], errors='coerce').astype('Int32')\n",
    "        df_cleaned['value'] = pd.to_numeric(df_cleaned['value'], errors='coerce').astype(np.float32)\n",
    "\n",
    "        # 3. Coerce datetime types\n",
    "        if 'created_at' in df_cleaned.columns:\n",
    "            df_cleaned['created_at'] = pd.to_datetime(df_cleaned['created_at'], errors='coerce')\n",
    "        if 'updated_at' in df_cleaned.columns:\n",
    "            df_cleaned['updated_at'] = pd.to_datetime(df_cleaned['updated_at'], errors='coerce')\n",
    "\n",
    "        # 4. Replace empty strings with NaN so they are properly ignored\n",
    "        df_cleaned = df_cleaned.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "        # 5. Convert other dtypes to best possible\n",
    "        df_cleaned = df_cleaned.convert_dtypes()\n",
    "        logger.info('Successfully cleaned DataFrame.')\n",
    "\n",
    "        # 6. Convert all string data to lowercase\n",
    "        df_cleaned = df_cleaned.applymap(lambda s: s.lower() if isinstance(s, str) else s)\n",
    "        logger.info('Converted all string data to lowercase.')\n",
    "        return df_cleaned\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f'An error occurred during DataFrame cleaning: {e}', exc_info=True)\n",
    "        return None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframes(dataframes, normalize_columns):\n",
    "    \"\"\"Normalizes a list of DataFrames by replacing name columns with foreign key IDs.\n",
    "\n",
    "    This function iterates through a list of dataframes and, for each one, iterates\n",
    "    through a dictionary of columns that need to be normalized. It uses the \n",
    "    `replace_name_with_id_df` utility to look up or create the corresponding ID\n",
    "    in the database.\n",
    "\n",
    "    Args:\n",
    "        dataframes (list[pd.DataFrame]): A list of DataFrames to normalize.\n",
    "        normalize_columns (dict): A dictionary mapping column names to SQLModel classes and attributes.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: The list of normalized DataFrames.\n",
    "    \"\"\"\n",
    "    logger.info(f'Starting normalization process for {len(dataframes)} dataframes.')\n",
    "    normalized_dfs = []\n",
    "    try:\n",
    "        with Session(engine) as db:\n",
    "            for i, df in enumerate(dataframes):\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    logger.warning(f'Item {i+1} is not a DataFrame, skipping.')\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f'Processing DataFrame #{i+1} with {len(df)} rows.')\n",
    "                df_normalized = df.copy()\n",
    "\n",
    "                for df_col, (model, model_name_attr) in normalize_columns.items():\n",
    "                    if df_col not in df_normalized.columns:\n",
    "                        logger.warning(f\"Column '{df_col}' not in DataFrame #{i+1}. Skipping normalization for this column.\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Skip normalization if the column is all NaN/None\n",
    "                        if df_normalized[df_col].isnull().all():\n",
    "                            logger.info(f\"Skipping normalization for column '{df_col}' as it contains only null values.\")\n",
    "                            continue\n",
    "                            \n",
    "                        logger.info(f\"Normalizing column '{df_col}' using model '{model.__name__}'.\")\n",
    "                        df_normalized, num_created = replace_name_with_id_df(\n",
    "                            db=db,\n",
    "                            df=df_normalized,\n",
    "                            ref_model=model,\n",
    "                            df_name_column=df_col,\n",
    "                            model_name_attr=model_name_attr,\n",
    "                            id_column_name='id',\n",
    "                            final_column_name=f'{df_col}_id'\n",
    "                        )\n",
    "                        if num_created > 0:\n",
    "                            logger.info(f\"Created {num_created} new records in '{model.__name__}' table.\")\n",
    "                        new_col_name = f'{df_col}_id'\n",
    "                        num_nulls = df_normalized[new_col_name].isnull().sum()\n",
    "                        logger.info(f\"Successfully normalized '{df_col}'. New column '{new_col_name}' contains {num_nulls} null values.\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error normalizing column '{df_col}' in DataFrame #{i+1}: {e}\", exc_info=True)\n",
    "                        continue # Continue to the next column\n",
    "                \n",
    "                normalized_dfs.append(df_normalized)\n",
    "                logger.info(f'Finished processing DataFrame #{i+1}.')\n",
    "            \n",
    "            logger.info('Committing database session.')\n",
    "            db.commit()\n",
    "            logger.info('Database commit successful.')\n",
    "    except Exception as e:\n",
    "        logger.error(f'A critical error occurred during the database session: {e}', exc_info=True)\n",
    "        db.rollback()\n",
    "        logger.info('Database session rolled back.')\n",
    "        \n",
    "    return normalized_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Execution Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 13:21:19,648 - INFO - Starting data extraction...\n",
      "2026-01-14 13:21:19,691 - INFO - HTTP Request: GET http://localhost:4200/api/admin/version \"HTTP/1.1 200 OK\"\n",
      "2026-01-14 13:21:19,692 - WARNING - Your Prefect server is running an older version of Prefect than your client which may result in unexpected behavior. Please upgrade your Prefect server from version 3.5.0 to version 3.6.7 or higher.\n",
      "2026-01-14 13:21:19,706 - INFO - Extracting raw data from 'Biodiesel_Plants.csv'...\n",
      "2026-01-14 13:21:19,729 - INFO - Attempting refresh to obtain initial access_token\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\_openssl_crypt.py:97: DeprecationWarning: sign() is deprecated. Use the equivalent APIs in cryptography.\n",
      "  return crypto.sign(self._key, message, 'sha256')\n",
      "2026-01-14 13:21:19,733 - INFO - Refreshing access_token\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\client.py:789: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  self.token_expiry = delta + _UTCNOW()\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\client.py:647: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = _UTCNOW()\n",
      "2026-01-14 13:21:20,805 - INFO - Successfully extracted raw data.\n",
      "2026-01-14 13:21:20,810 - INFO - Finished in state Completed()\n",
      "2026-01-14 13:21:20,841 - INFO - HTTP Request: GET http://localhost:4200/api/admin/version \"HTTP/1.1 200 OK\"\n",
      "2026-01-14 13:21:20,842 - WARNING - Your Prefect server is running an older version of Prefect than your client which may result in unexpected behavior. Please upgrade your Prefect server from version 3.5.0 to version 3.6.7 or higher.\n",
      "2026-01-14 13:21:20,851 - INFO - Extracting raw data from 'CA_proc_points.zip'...\n",
      "2026-01-14 13:21:20,871 - INFO - Attempting refresh to obtain initial access_token\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\_openssl_crypt.py:97: DeprecationWarning: sign() is deprecated. Use the equivalent APIs in cryptography.\n",
      "  return crypto.sign(self._key, message, 'sha256')\n",
      "2026-01-14 13:21:20,873 - INFO - Refreshing access_token\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\client.py:789: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  self.token_expiry = delta + _UTCNOW()\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\client.py:647: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = _UTCNOW()\n",
      "2026-01-14 13:21:22,027 - INFO - Successfully extracted raw data.\n",
      "2026-01-14 13:21:22,034 - INFO - Finished in state Completed()\n",
      "2026-01-14 13:21:22,069 - INFO - HTTP Request: GET http://localhost:4200/api/admin/version \"HTTP/1.1 200 OK\"\n",
      "2026-01-14 13:21:22,070 - WARNING - Your Prefect server is running an older version of Prefect than your client which may result in unexpected behavior. Please upgrade your Prefect server from version 3.5.0 to version 3.6.7 or higher.\n",
      "2026-01-14 13:21:22,082 - INFO - Extracting raw data from 'US_Petroleum_Pipelines.geojson'...\n",
      "2026-01-14 13:21:22,105 - INFO - Attempting refresh to obtain initial access_token\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\_openssl_crypt.py:97: DeprecationWarning: sign() is deprecated. Use the equivalent APIs in cryptography.\n",
      "  return crypto.sign(self._key, message, 'sha256')\n",
      "2026-01-14 13:21:22,108 - INFO - Refreshing access_token\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\client.py:789: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  self.token_expiry = delta + _UTCNOW()\n",
      "C:\\Users\\Abigail\\OneDrive\\Documents\\GitHub\\ca-biositing\\.pixi\\envs\\default\\Lib\\site-packages\\oauth2client\\client.py:647: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = _UTCNOW()\n",
      "2026-01-14 13:21:23,976 - INFO - Successfully extracted raw data.\n",
      "2026-01-14 13:21:23,981 - INFO - Finished in state Completed()\n",
      "2026-01-14 13:21:24,019 - INFO - Data extraction complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                  company  bbi_index             city  \\\n",
      "0                     American GreenFuels        NaN        New Haven   \n",
      "1                Down To Earth Energy LLC        NaN           Monroe   \n",
      "2                      Maine Bio-Fuel Inc        NaN         Portland   \n",
      "3                   Cape Cod Biofuels Inc        NaN         Sandwich   \n",
      "4             Renewable Fuels by Peterson        NaN  North Haverhill   \n",
      "..                                    ...        ...              ...   \n",
      "73                     Walsh BioFuels LLC       58.0          Mauston   \n",
      "74  Western Iowa Energy - Agron Bioenergy       60.0      Watsonville   \n",
      "75           White Mountain Biodiesel LLC       62.0  North Haverhill   \n",
      "76                 World Energy - Natchez       65.0          Natchez   \n",
      "77                    World Energy - Rome       66.0             Rome   \n",
      "\n",
      "            state  capacity_mmg_per_y            feedstock       status  \\\n",
      "0     Connecticut                  35                  NaN          NaN   \n",
      "1         Georgia                   2                  NaN          NaN   \n",
      "2           Maine                   1                  NaN          NaN   \n",
      "3   Massachusetts                   1                  NaN          NaN   \n",
      "4   New Hampshire                   8                  NaN          NaN   \n",
      "..            ...                 ...                  ...          ...   \n",
      "73             WI                   5  Distillers corn oil  Operational   \n",
      "74             CA                  15       Multifeedstock  Operational   \n",
      "75             NH                   8        Yellow grease  Operational   \n",
      "76             MS                  72       Vegetable oils  Operational   \n",
      "77             GA                  20       Multifeedstock  Operational   \n",
      "\n",
      "                                         address  \\\n",
      "0                                            NaN   \n",
      "1                                            NaN   \n",
      "2                                            NaN   \n",
      "3                                            NaN   \n",
      "4                                            NaN   \n",
      "..                                           ...   \n",
      "73         WI-80 & Mauston Rd, Mauston, WI 53948   \n",
      "74         860 W Beach St, Watsonville, CA 95076   \n",
      "75  35 Business Pk Rd, North Haverhill, NH 03774   \n",
      "76               L E Barry Rd, Natchez, MS 39120   \n",
      "77         555 W Hermitage Rd NE, Rome, GA 30161   \n",
      "\n",
      "                               coordinates   latitude   longitude  \\\n",
      "0                                      NaN  41.290100  -72.902900   \n",
      "1                                      NaN  33.757170  -83.727700   \n",
      "2                                      NaN  43.691400  -70.328100   \n",
      "3                                      NaN  41.717700  -70.484500   \n",
      "4                                      NaN  44.077000  -72.004700   \n",
      "..                                     ...        ...         ...   \n",
      "73  43.777703209506015, -90.05515309603231  43.777703  -90.055153   \n",
      "74  36.90388067222262, -121.76968983169887  36.903881 -121.769690   \n",
      "75    44.07671166339629, -72.0049135970569  44.076712  -72.004914   \n",
      "76   31.533227585047626, -91.4374585606938  31.533228  -91.437459   \n",
      "77   34.327173247943826, -85.0919307299404  34.327173  -85.091931   \n",
      "\n",
      "                                               source  \n",
      "0   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "1   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "2   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "3   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "4   https://atlas.eia.gov/datasets/79dad60ce89c475...  \n",
      "..                                                ...  \n",
      "73  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "74  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "75  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "76  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "77  https://issuu.com/bbiinternational/docs/biodie...  \n",
      "\n",
      "[78 rows x 12 columns],                  Address      COUNTY         City  \\\n",
      "0         327 Daniels Ln        Kern  Bakersfield   \n",
      "1      26060 Colusa Road      Fresno     Coalinga   \n",
      "2    21490 Ortigalita Rd      Merced    Los Banos   \n",
      "3      8800 S Minturn Rd      Merced     Le Grand   \n",
      "4                    NaN  Stanislaus          NaN   \n",
      "..                   ...         ...          ...   \n",
      "707      3230 Oakdale Rd           0  Paso Robles   \n",
      "708     5963 Graham Ct D           0    Livermore   \n",
      "709       708 Addison St           0     Berkeley   \n",
      "710          9275 Midway           0       Durham   \n",
      "711         95 Booker Rd           0    Templeton   \n",
      "\n",
      "                           Company  Join_Count MASTERTYPE State Subtype  \\\n",
      "0           Real Almond Processing         1.0     almond    CA  almond   \n",
      "1                     Harris Woolf         1.0     almond    CA  almond   \n",
      "2       Parreira Almond Processing         1.0     almond    CA  almond   \n",
      "3               Minturn Nut Co Inc         1.0     almond    CA  almond   \n",
      "4                    Tree Nuts LLC         1.0     almond    CA  almond   \n",
      "..                             ...         ...        ...   ...     ...   \n",
      "707               Red Soles Winery         0.0     winery    CA  pomace   \n",
      "708           Eckert Estate Winery         0.0     winery    CA  pomace   \n",
      "709            Takara Sake USA Inc         0.0     winery    CA  pomace   \n",
      "710  Bertagna Son Kissed Vineyards         0.0     winery    CA  pomace   \n",
      "711          Pasoport Wine Company         0.0     winery    CA  pomace   \n",
      "\n",
      "     TARGET_FID      Type    Zip  TYPE  \\\n",
      "0           0.0  shelling  93307  PROC   \n",
      "1           1.0  shelling  93210  PROC   \n",
      "2           2.0  shelling  93635  PROC   \n",
      "3           3.0  shelling  95333  PROC   \n",
      "4           4.0  shelling      0  PROC   \n",
      "..          ...       ...    ...   ...   \n",
      "707         0.0    winery  93446  PROC   \n",
      "708         0.0    winery  94550  PROC   \n",
      "709         0.0    winery  94710  PROC   \n",
      "710         0.0    winery  95938  PROC   \n",
      "711         0.0    winery  93465  PROC   \n",
      "\n",
      "                                              wkt_geom  \\\n",
      "0    0101000020E610000090A6D5EBF1BF5DC0D652E5208DAC...   \n",
      "1    0101000020E6100000931B45D61A175EC0EC67B114C915...   \n",
      "2    0101000020E610000008951348E4375EC00937B62ACB7D...   \n",
      "3    0101000020E6100000E025DD3B8F115EC0B26066C8C694...   \n",
      "4    0101000020E61000007CD3F4D901365EC07E37DDB243BE...   \n",
      "..                                                 ...   \n",
      "707  0101000020E61000005029C0C128305EC0723788D68AC8...   \n",
      "708  0101000020E6100000A2F8E758146E5EC0DCEB4905BAD6...   \n",
      "709  0101000020E61000003B78DCA62E935EC0887A66EEC6EE...   \n",
      "710  0101000020E6100000DAB6DE2527735EC042CAF4F00AD2...   \n",
      "711  0101000020E610000051700C5C792F5EC030FFB661F3C7...   \n",
      "\n",
      "                                 geom   latitude   longitude  \n",
      "0     POINT (-118.9991407 35.3480569)  35.348057 -118.999141  \n",
      "1       POINT (-120.361013 36.170199)  36.170199 -120.361013  \n",
      "2    POINT (-120.8733082 36.98276266)  36.982763 -120.873308  \n",
      "3    POINT (-120.2743673 37.16231637)  37.162316 -120.274367  \n",
      "4       POINT (-120.843863 37.486441)  37.486441 -120.843863  \n",
      "..                                ...        ...         ...  \n",
      "707    POINT (-120.7524876 35.566737)  35.566737 -120.752488  \n",
      "708   POINT (-121.7199919 37.6775519)  37.677552 -121.719992  \n",
      "709   POINT (-122.2997224 37.8654459)  37.865446 -122.299722  \n",
      "710   POINT (-121.7992644 39.6409589)  39.640959 -121.799264  \n",
      "711  POINT (-120.7417822 35.56211492)  35.562115 -120.741782  \n",
      "\n",
      "[712 rows x 16 columns],       OBJECTID                 Opername              Pipename Source  \\\n",
      "0            1            NUSTAR ENERGY       Valley Pipeline    EIA   \n",
      "1            2  TRANSMONTAIGNE PARTNERS  Diamondback Pipeline    EIA   \n",
      "2            3            NUSTAR ENERGY       Valley Pipeline    EIA   \n",
      "3            4  TRANSMONTAIGNE PARTNERS  Diamondback Pipeline    EIA   \n",
      "4            5  TRANSMONTAIGNE PARTNERS  Diamondback Pipeline    EIA   \n",
      "...        ...                      ...                   ...    ...   \n",
      "4307      4308                     None                  None   None   \n",
      "4308      4309                     None                  None   None   \n",
      "4309      4310                     None                  None   None   \n",
      "4310      4311                     None                  None   None   \n",
      "4311      4312                     None                  None   None   \n",
      "\n",
      "                   Type Notes  ARTIFICIAL  MASTER_OID           commodity  \\\n",
      "0     Petroleum Product  None           0       299.0  Petroleum Products   \n",
      "1     Petroleum Product  None           0       300.0  Petroleum Products   \n",
      "2     Petroleum Product  None           0       301.0  Petroleum Products   \n",
      "3     Petroleum Product  None           0       302.0  Petroleum Products   \n",
      "4     Petroleum Product  None           0       303.0  Petroleum Products   \n",
      "...                 ...   ...         ...         ...                 ...   \n",
      "4307               None  None           2         NaN  Petroleum Products   \n",
      "4308               None  None           2         NaN  Petroleum Products   \n",
      "4309               None  None           2         NaN  Petroleum Products   \n",
      "4310               None  None           2         NaN  Petroleum Products   \n",
      "4311               None  None           2         NaN  Petroleum Products   \n",
      "\n",
      "     Volume Capacity   VCR  Shape_Length      Mode_Type    Length  \\\n",
      "0      None     None  None   1496.712523  pipeline_prod  0.930012   \n",
      "1      None     None  None   1934.748751  pipeline_prod  1.202195   \n",
      "2      None     None  None     55.870089  pipeline_prod  0.034716   \n",
      "3      None     None  None   2082.004459  pipeline_prod  1.293695   \n",
      "4      None     None  None   9904.740005  pipeline_prod  6.154508   \n",
      "...     ...      ...   ...           ...            ...       ...   \n",
      "4307   None     None  None    207.259275  pipeline_prod  0.128785   \n",
      "4308   None     None  None    242.718991  pipeline_prod  0.150818   \n",
      "4309   None     None  None    592.947623  pipeline_prod  0.368440   \n",
      "4310   None     None  None    836.752423  pipeline_prod  0.519933   \n",
      "4311   None     None  None   7215.035750  pipeline_prod  4.483206   \n",
      "\n",
      "                                               geometry  \n",
      "0     MULTILINESTRING ((-97.38949 25.96387, -97.3921...  \n",
      "1     MULTILINESTRING ((-97.38299 25.96636, -97.3839...  \n",
      "2     MULTILINESTRING ((-97.39214 25.97691, -97.3922...  \n",
      "3     MULTILINESTRING ((-97.38165 25.9667, -97.38265...  \n",
      "4     MULTILINESTRING ((-97.52187 26.00233, -97.5283...  \n",
      "...                                                 ...  \n",
      "4307  MULTILINESTRING ((-93.19672 45.02409, -93.1989...  \n",
      "4308  MULTILINESTRING ((-93.19672 45.02409, -93.1997...  \n",
      "4309  MULTILINESTRING ((-93.19672 45.02409, -93.2032...  \n",
      "4310  MULTILINESTRING ((-93.19672 45.02409, -93.1966...  \n",
      "4311  MULTILINESTRING ((-93.19672 45.02409, -93.2638...  \n",
      "\n",
      "[4312 rows x 16 columns]]\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Extraction ---\n",
    "# In a real Prefect flow, each extraction would be a separate task.\n",
    "logger.info('Starting data extraction...')\n",
    "biodiesel_plants_df = biodiesel_plants.extract(project_root=project_root)\n",
    "proc_points_df = ca_proc_points.extract(project_root=project_root)\n",
    "petro_pipelines_df = petroleum_pipelines.extract(project_root=project_root)\n",
    "dataframes = [biodiesel_plants_df, proc_points_df, petro_pipelines_df]\n",
    "print(dataframes)\n",
    "logger.info('Data extraction complete.')\n",
    "\n",
    "# # --- 2. Cleaning ---\n",
    "# # This list comprehension applies the cleaning function to each extracted dataframe.\n",
    "# logger.info('Starting data cleaning...')\n",
    "# clean_dataframes = [clean_the_gsheets(df) for df in dataframes if df is not None]\n",
    "# logger.info('Data cleaning complete.')\n",
    "\n",
    "# # --- 3. Normalization ---\n",
    "# # This dictionary defines the columns to be normalized. \n",
    "# # The key is the column name in the DataFrame.\n",
    "# # The value is a tuple containing the corresponding SQLAlchemy model and the name of the attribute on the model to match against.\n",
    "# NORMALIZE_COLUMNS = {\n",
    "#     'resource': (Resource, 'name'),\n",
    "#     'prepared_sample': (PreparedSample, 'name'),\n",
    "#     'preparation_method': (PreparationMethod, 'name'),\n",
    "#     'parameter': (Parameter, 'name'),\n",
    "#     'unit': (Unit, 'name'),\n",
    "#     'analyst_email': (Contact, 'email'),\n",
    "#     'analysis_type': (AnalysisType, 'name'),\n",
    "#     'primary_ag_product': (PrimaryAgProduct, 'name')\n",
    "# }\n",
    "\n",
    "# logger.info('Starting data normalization...')\n",
    "# normalized_dataframes = normalize_dataframes(clean_dataframes, NORMALIZE_COLUMNS)\n",
    "# logger.info('Data normalization complete.')\n",
    "\n",
    "# # --- 4. Display Results ---\n",
    "# logger.info('Displaying results of normalization...')\n",
    "# for i, df in enumerate(normalized_dataframes):\n",
    "#     print(f'--- Normalized DataFrame {i+1} ---')\n",
    "#     display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Plan\n",
    "\n",
    "The code in this notebook will be transitioned to the main ETL pipeline by following these steps:\n",
    "\n",
    "1.  **Function Migration**: The `clean_the_gsheets` and `normalize_dataframes` functions will be moved to a new utility module, for example, `src/ca_biositing/pipeline/ca_biositing/pipeline/utils/etl_utils.py`. Each function will be decorated with `@task` from Prefect to turn it into a reusable pipeline component.\n",
    "2.  **Flow Creation**: A new Prefect flow will be created in the `src/ca_biositing/pipeline/ca_biositing/pipeline/flows/` directory (e.g., `master_extraction_flow.py`). This flow will orchestrate the entire ETL process for a given data source.\n",
    "3.  **Task Integration**: The new flow will be composed of individual tasks. It will call the existing extraction tasks (`proximate.extract`, etc.), and then pass the results to the new cleaning and normalization tasks from `etl_utils.py`.\n",
    "4.  **Logging**: The `logging` module will be replaced with `get_run_logger()` from Prefect within the tasks to ensure logs are captured by the Prefect UI.\n",
    "5.  **Configuration**: The `NORMALIZE_COLUMNS` dictionary will be moved to a configuration file or defined within the relevant flow to make it easier to manage and modify without changing the code.\n",
    "6.  **Testing**: Unit tests will be written for the new utility functions in `etl_utils.py`. An integration test will be created for the new Prefect flow to ensure all the tasks work together correctly.\n",
    "7.  **Deployment**: Once the flow is complete and tested, it will be deployed to the Prefect server using the `pixi run deploy` command, making it available to be run on a schedule or manually via the UI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
