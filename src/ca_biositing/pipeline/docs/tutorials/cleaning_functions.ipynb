{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "87bf3c52",
      "metadata": {},
      "source": [
        "# Cleaning Helpers\n",
        "This notebook documents reusable data-cleaning helper functions for the CA Biositing ETL. Each helper is documented with purpose, behaviour, and examples so junior engineers can adopt them safely.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19094f99",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports used by all helpers\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Note: these helpers use `janitor.clean_names()`; ensure `janitor` is available when running interactively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44300a16",
      "metadata": {},
      "source": [
        "## 1) `clean_names_df` — standardize column names\n",
        "Purpose: make column names predictable (snake_case, no spaces) so downstream code can reference columns reliably.\n",
        "When to use: immediately after loading raw data from sheets/CSV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad623319",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_names_df(df):\n",
        "    \"\"\"Return a copy of `df` with cleaned column names using `janitor.clean_names()`.\n",
        "\n",
        "    Behavior:\n",
        "    - If `df` is not a DataFrame, logs and returns the original value.\n",
        "    - Returns a new DataFrame reference (does not mutate input).\n",
        "\n",
        "    Example:\n",
        "        df2 = clean_names_df(df)\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        logger.error('clean_names_df: input is not a DataFrame')\n",
        "        return df\n",
        "    # janitor.clean_names normalizes casing, removes punctuation/spaces, and converts to snake_case\n",
        "    return df.clean_names()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c49ae10",
      "metadata": {},
      "source": [
        "## 2) `replace_empty_with_na` — convert empty / whitespace-only strings to NaN\n",
        "Purpose: make empty strings behave like missing values so pandas functions (dropna, isna) work as expected.\n",
        "Parameters:\n",
        "- `columns`: list of columns to target; `None` means all columns.\n",
        "- `regex`: regex used to match empty/whitespace strings (default `'^\\s*$'`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34e8787",
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_empty_with_na(df, columns=None, regex=r'^\\s*$'):\n",
        "    \"\"\"Replace empty/whitespace-only strings with `np.nan`.\n",
        "\n",
        "    Returns a new DataFrame; if `columns` specified, only those columns are replaced.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        logger.error('replace_empty_with_na: input is not a DataFrame')\n",
        "        return df\n",
        "    if columns is None:\n",
        "        # operate on entire DataFrame (string matches only affect string-like cells)\n",
        "        return df.replace(regex, np.nan, regex=True)\n",
        "    df = df.copy()\n",
        "    # Limit replacement to the requested columns (if they exist)\n",
        "    cols = [c for c in columns if c in df.columns]\n",
        "    if not cols:\n",
        "        logger.warning('replace_empty_with_na: no matching columns found; returning original DataFrame')\n",
        "        return df\n",
        "    df[cols] = df[cols].replace(regex, np.nan, regex=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e70ede1",
      "metadata": {},
      "source": [
        "## 3) `to_lowercase_df` — normalize string values to lowercase\n",
        "Purpose: reduce variations in human-provided text (e.g., 'Corn', 'corn', 'CORN') to a single canonical form.\n",
        "Notes: preserves missing values (NaN) and only acts on string-like columns. You can pass a subset of columns to limit changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27ec74f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_lowercase_df(df, columns=None):\n",
        "    \"\"\"Lowercase string columns.\n",
        "\n",
        "    - `columns` selects which columns to lowercase; default is all object/string dtype columns.\n",
        "    Returns a new DataFrame.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        logger.error('to_lowercase_df: input is not a DataFrame')\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    if columns is None:\n",
        "        str_cols = df.select_dtypes(include=['object', 'string']).columns\n",
        "    else:\n",
        "        str_cols = [c for c in columns if c in df.columns]\n",
        "    for c in str_cols:\n",
        "        # convert to pandas string dtype to get vectorized string methods; preserve NaN values\n",
        "        df[c] = df[c].astype('string').str.lower().where(df[c].notna(), df[c])\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9d31cdf",
      "metadata": {},
      "source": [
        "## 4) `standard_clean` — composed pipeline for common cleaning steps\n",
        "Purpose: provide a one-line convenience wrapper that composes name-cleaning, empty->NA replacement, lowercase conversion, and dtype inference.\n",
        "When to use: for quick interactive cleaning or as a default pre-processing step in flows. For strict per-column coercions, use the coercion helpers below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f009f323",
      "metadata": {},
      "outputs": [],
      "source": [
        "def standard_clean(df, lowercase=True, replace_empty=True):\n",
        "    \"\"\"Run a standard cleaning sequence and return the cleaned DataFrame.\n",
        "\n",
        "    Steps:\n",
        "    1. `clean_names_df`\n",
        "    2. `replace_empty_with_na` (optional)\n",
        "    3. `to_lowercase_df` (optional)\n",
        "    4. `convert_dtypes()` to let pandas pick better dtypes\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        logger.error('standard_clean: input is not a DataFrame')\n",
        "        return None\n",
        "    df = clean_names_df(df)\n",
        "    if replace_empty:\n",
        "        df = replace_empty_with_na(df)\n",
        "    if lowercase:\n",
        "        df = to_lowercase_df(df)\n",
        "    # convert_dtypes helps for nullable integers, booleans, and strings\n",
        "    df = df.convert_dtypes()\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be5da14c",
      "metadata": {},
      "source": [
        "## 5) Coercion helpers — targeted, explicit type conversions\n",
        "Rationale: `standard_clean` calls `convert_dtypes()` which is useful, but for production ETL we need deterministic per-column coercion (Int64 nullable ints, float32 for numeric, datetime parsing, geometry parsing, etc.).\n",
        "This section provides helpers for common coercion targets and a single `coerce_columns` wrapper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4ece8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _coerce_int(df, cols):\n",
        "    \"\"\"Coerce listed columns to pandas nullable `Int64`. Non-parsable values become <NA>.\n",
        "    \"\"\"\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce').astype('Int64')\n",
        "    return df\n",
        "\n",
        "def _coerce_float(df, cols, float_dtype=np.float32):\n",
        "    \"\"\"Coerce columns to float (default `np.float32`) with errors coerced to NaN.\n",
        "    \"\"\"\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce').astype(float_dtype)\n",
        "    return df\n",
        "\n",
        "def _coerce_datetime(df, cols, **kwargs):\n",
        "    \"\"\"Parse datetime-like columns using `pd.to_datetime`. Extra kwargs (e.g., `dayfirst`) are passed through.\n",
        "    \"\"\"\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_datetime(df[c], errors='coerce', **kwargs)\n",
        "    return df\n",
        "\n",
        "def _coerce_bool(df, cols):\n",
        "    \"\"\"Coerce common boolean encodings to pandas nullable boolean dtype.\n",
        "    Accepts True/False, 'true'/'false', '1'/'0'. Unknowns become <NA>.\n",
        "    \"\"\"\n",
        "    mapping = {True: True, False: False, 'true': True, 'false': False, '1': True, '0': False}\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].map(mapping).astype('boolean')\n",
        "    return df\n",
        "\n",
        "def _coerce_category(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].astype('category')\n",
        "    return df\n",
        "\n",
        "def _coerce_geometry(df, cols, geom_format='wkt'):\n",
        "    \"\"\"Attempt to convert WKT strings to shapely geometry objects. If `shapely` is not installed, logs a warning and skips geometry coercion.\n",
        "    Supported `geom_format`: currently only 'wkt'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from shapely import wkt\n",
        "    except Exception:\n",
        "        logger.warning('shapely not available; geometry coercion skipped')\n",
        "        return df\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].apply(lambda v: wkt.loads(v) if isinstance(v, str) and v.strip() else None)\n",
        "    return df\n",
        "\n",
        "def coerce_columns(df,\n",
        "                   int_cols=None,\n",
        "                   float_cols=None,\n",
        "                   datetime_cols=None,\n",
        "                   bool_cols=None,\n",
        "                   category_cols=None,\n",
        "                   geometry_cols=None,\n",
        "                   dtype_map=None,\n",
        "                   float_dtype=np.float32):\n",
        "    \"\"\"Top-level wrapper to coerce groups of columns.\n",
        "\n",
        "    - `dtype_map` is an alternative mapping where keys are 'int','float','datetime','bool','category','geometry'\n",
        "      and values are lists of column names. Explicit keyword lists override `dtype_map` entries.\n",
        "    Returns a new DataFrame with coerced columns where possible.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        logger.error('coerce_columns: input is not a DataFrame')\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    if dtype_map:\n",
        "        int_cols = int_cols or dtype_map.get('int') or dtype_map.get('integer')\n",
        "        float_cols = float_cols or dtype_map.get('float')\n",
        "        datetime_cols = datetime_cols or dtype_map.get('datetime') or dtype_map.get('date')\n",
        "        bool_cols = bool_cols or dtype_map.get('bool')\n",
        "        category_cols = category_cols or dtype_map.get('category')\n",
        "        geometry_cols = geometry_cols or dtype_map.get('geometry')\n",
        "\n",
        "    if int_cols:\n",
        "        df = _coerce_int(df, int_cols)\n",
        "    if float_cols:\n",
        "        df = _coerce_float(df, float_cols, float_dtype)\n",
        "    if datetime_cols:\n",
        "        df = _coerce_datetime(df, datetime_cols)\n",
        "    if bool_cols:\n",
        "        df = _coerce_bool(df, bool_cols)\n",
        "    if category_cols:\n",
        "        df = _coerce_category(df, category_cols)\n",
        "    if geometry_cols:\n",
        "        df = _coerce_geometry(df, geometry_cols)\n",
        "\n",
        "    return df\n",
        "\n",
        "def coerce_columns_list(dfs, **coerce_kwargs):\n",
        "    \"\"\"Apply `coerce_columns` to a list of dataframes and return the results in the same order.\n",
        "    Non-DataFrame items are preserved (with a warning).\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for i, df in enumerate(dfs):\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            logger.warning(f'Item {i} is not a DataFrame; skipping')\n",
        "            out.append(df)\n",
        "            continue\n",
        "        out.append(coerce_columns(df.copy(), **coerce_kwargs))\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "564ff4f6",
      "metadata": {},
      "source": [
        "## 6) Usage examples\n",
        "These snippets demonstrate typical workflows; run them interactively with sample DataFrames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860f32d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: a raw dataframe from Google Sheets (simulate)\n",
        "raw = pd.DataFrame({\n",
        "    'Sample No': ['1', '2', '  ', '4'],\n",
        "    'Value': ['10.0', 'NaN', '', '7.5'],\n",
        "    'Resource': ['Corn', 'Wheat', 'corn', None],\n",
        "    'Created At': ['2020-01-01', '2020-02-01', '', 'not-a-date']\n",
        "})\n",
        "\n",
        "# 1) Standard clean: names, empty->NA, lowercase, dtype inference\n",
        "cleaned = standard_clean(raw)\n",
        "print(cleaned.dtypes)\n",
        "display(cleaned)\n",
        "\n",
        "# 2) Explicit coercion\n",
        "coerced = coerce_columns(cleaned,\n",
        "                         int_cols=['sample_no'],\n",
        "                         float_cols=['value'],\n",
        "                         datetime_cols=['created_at'])\n",
        "print(coerced.dtypes)\n",
        "display(coerced)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed6cb452",
      "metadata": {},
      "source": [
        "## Notes & Next steps\n",
        "- Geometry coercion requires `shapely`: to add it for the pipeline environment use Pixi (see `pixi.toml`). Example (local):\n",
        "\n",
        "```bash\n",
        "pixi add --feature pipeline --pypi shapely\n",
        "pixi install\n",
        "```\n",
        "\n",
        "- For production use we should move these helpers into `src/ca_biositing/pipeline/.../etl_utils.py` and decorate Prefect tasks where appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9c76c4",
      "metadata": {},
      "source": [
        "## 7) GeoPandas Integration for Shapefiles\n",
        "\n",
        "For geospatial work, **geopandas** is the preferred approach to load and work with shapefiles and other vector data formats. Geopandas extends pandas DataFrames with a `geometry` column containing shapely geometry objects.\n",
        "\n",
        "### Loading Shapefiles with GeoPandas\n",
        "\n",
        "Instead of parsing WKT strings, you can load shapefiles directly:\n",
        "\n",
        "```python\n",
        "import geopandas as gpd\n",
        "\n",
        "# Load a shapefile (creates a GeoDataFrame with geometry column)\n",
        "gdf = gpd.read_file('path/to/shapefile.shp')\n",
        "\n",
        "# The geometry column is already properly typed\n",
        "print(gdf.geometry.dtype)  # geometry\n",
        "print(type(gdf.geometry[0]))  # shapely.geometry object\n",
        "\n",
        "# You can then merge this with your cleaned data\n",
        "# or use spatial operations (intersects, buffer, distance, etc.)\n",
        "```\n",
        "\n",
        "### Coercing Geometry from GeoDataFrames\n",
        "\n",
        "When you load a GeoDataFrame, the geometry column is already properly typed. Use `geometry_format='geodataframe'` to skip coercion:\n",
        "\n",
        "```python\n",
        "# After loading: gdf = gpd.read_file('shapefile.shp')\n",
        "gdf_clean = standard_clean(gdf)  # clean as normal\n",
        "\n",
        "# Coerce other columns but skip geometry (already correct type)\n",
        "gdf_coerced = coerce_columns(\n",
        "    gdf_clean,\n",
        "    int_cols=['id'],\n",
        "    float_cols=['area'],\n",
        "    geometry_cols=['geometry'],\n",
        "    geometry_format='geodataframe'  # Skip coercion; geometry is already GeoSeries\n",
        ")\n",
        "```\n",
        "\n",
        "### Mixed Workflows: GeoDataFrames + Tabular Data\n",
        "\n",
        "A common pattern for bioeconomy site selection:\n",
        "\n",
        "```python\n",
        "# 1. Load tabular data (Google Sheets)\n",
        "df = pd.read_csv('sites.csv')  # columns: site_id, site_name, resource, value, etc.\n",
        "df_clean = standard_clean(df)\n",
        "df_coerced = coerce_columns(df_clean, int_cols=['site_id'], float_cols=['value'])\n",
        "\n",
        "# 2. Load spatial data (shapefiles)\n",
        "gdf = gpd.read_file('parcel_boundaries.shp')  # geometry column with polygons\n",
        "\n",
        "# 3. Merge on spatial relationship (e.g., which parcels contain which sites)\n",
        "# Use sjoin for spatial join\n",
        "merged = gpd.sjoin(gdf, gpd.GeoDataFrame(geometry=gpd.points_from_xy(df_coerced['lon'], df_coerced['lat'])))\n",
        "\n",
        "# 4. Now you have a GeoDataFrame with tabular + spatial info\n",
        "print(merged.head())\n",
        "```\n",
        "\n",
        "### Setup: Adding GeoPandas to Your Environment\n",
        "\n",
        "GeoPandas is included in the `vector` feature. Enable it:\n",
        "\n",
        "```bash\n",
        "pixi install --feature vector\n",
        "# or for all GIS tools\n",
        "pixi install --feature gis\n",
        "```\n",
        "\n",
        "This provides: `geopandas`, `shapely`, `pyproj`, and other geospatial libraries needed for shapefile work.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
