{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Notebook for CA Biositing Project\n",
    "\n",
    "This notebook provides a documented walkthrough of the ETL (Extract, Transform, Load) process for the CA Biositing project. It is designed for interactive development and exploration before migrating logic into the production pipeline.\n",
    "\n",
    "It covers:\n",
    "\n",
    "1.  **Setup**: Importing necessary libraries and establishing a connection to the database.\n",
    "2.  **Extraction**: Pulling raw data from Google Sheets.\n",
    "3.  **Cleaning**: Standardizing data types, handling missing values, and cleaning column names.\n",
    "4.  **Normalization**: Replacing human-readable names (e.g., \"Corn\") with database foreign key IDs (e.g., `resource_id: 1`).\n",
    "5.  **Utilities**: Common functions for data manipulation and analysis.\n",
    "6.  **Deployment Plan**: A step-by-step guide for moving the code from this notebook into the production ETL modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport janitor as jn\nimport logging\nfrom IPython.display import display\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import select\n\n# --- Basic Logging Configuration for Notebook ---\n# When running in a notebook, we use Python's standard logging.\n# In the production pipeline, this will be replaced by Prefect's `get_run_logger()`\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger()\n\n# --- Import project modules ---\n# All namespace packages are installed in the Pixi environment.\n# Use the Pixi kernel in Jupyter to ensure correct imports.\ntry:\n    from ca_biositing.pipeline.utils.engine import engine\n    from ca_biositing.datamodels.schemas.generated.ca_biositing import *\n    from ca_biositing.pipeline.utils.name_id_swap import replace_name_with_id_df\n    from ca_biositing.pipeline.etl.extract import proximate, ultimate, cmpana, samplemetadata\n    logger.info('Successfully imported all project modules.')\nexcept ImportError as e:\n    logger.error(f'Failed to import project modules: {e}', exc_info=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the refactored cleaning/coercion helpers from the new package\nfrom ca_biositing.pipeline.utils.cleaning_functions import standard_clean, coerce_columns, coerce_columns_list\n\ndef clean_the_gsheets(df, lowercase=True, replace_empty=True):\n    \"\"\"Wrapper that applies the standardized cleaning pipeline implemented in `cleaning_functions`.\"\"\"\n    logger.info('Starting DataFrame cleaning via standard_clean.')\n    if not isinstance(df, pd.DataFrame):\n        logger.error('Input is not a pandas DataFrame.')\n        return None\n    try:\n        # Run the composed standard clean (names, empty->NA, lowercase, convert_dtypes)\n        df_cleaned = standard_clean(df, lowercase=lowercase, replace_empty=replace_empty)\n        # Preserve behaviour: drop rows missing key columns if present\n        subset = [c for c in ['resource', 'value'] if c in df_cleaned.columns]\n        if subset:\n            df_cleaned = df_cleaned.dropna(subset=subset)\n        logger.info(f'Cleaning complete; rows remaining: {len(df_cleaned)}')\n        return df_cleaned\n    except Exception as e:\n        logger.error(f'An error occurred during DataFrame cleaning: {e}', exc_info=True)\n        return None\n\n\n# --- Coercion Configuration Templates ---\n# You can define column coercions in two ways: explicit keyword arguments or a dtype_map dictionary.\n# For geometry: use geopandas to load shapefiles (geometry column is already properly typed).\n# Only use geometry_cols if you have WKT strings to parse.\n\n# APPROACH 1: Explicit keyword arguments (clear and direct)\nCOERCION_CONFIG_EXPLICIT = {\n    'int_cols': ['repl_no', 'sample_no'],\n    'float_cols': ['value', 'measurement'],\n    'datetime_cols': ['created_at', 'updated_at'],\n    'bool_cols': ['is_valid'],\n    'category_cols': ['status'],\n    'geometry_cols': []  # Use only if you have WKT strings; prefer geopandas for shapefiles\n}\n\n# APPROACH 2: dtype_map dictionary (compact, useful for dynamic configs)\nCOERCION_CONFIG_DTYPE_MAP = {\n    'int': ['repl_no', 'sample_no'],\n    'float': ['value', 'measurement'],\n    'datetime': ['created_at', 'updated_at'],\n    'bool': ['is_valid'],\n    'category': ['status'],\n    'geometry': []  # Use only if you have WKT strings; prefer geopandas for shapefiles\n}\n\n# APPROACH 3: GeoPandas GeoDataFrame (for shapefiles and spatial data)\n# When loading shapefiles with geopandas, geometry is already a GeoSeries.\n# Use geometry_format='geodataframe' to skip coercion:\nGEOPANDAS_CONFIG = {\n    'int_cols': ['id', 'repl_no'],\n    'float_cols': ['area', 'value'],\n    'geometry_cols': ['geometry'],\n    'geometry_format': 'geodataframe'  # Don't convert; already properly typed\n}\n\n# Usage: coerce_the_gsheets(df, **COERCION_CONFIG_EXPLICIT)\n#   or: coerce_the_gsheets(df, dtype_map=COERCION_CONFIG_DTYPE_MAP)\n#   or: coerce_the_gsheets(gdf, **GEOPANDAS_CONFIG)  # for GeoDataFrames\n\n\ndef coerce_the_gsheets(df, dtype_map=None, int_cols=None, float_cols=None, datetime_cols=None, bool_cols=None, category_cols=None, geometry_cols=None, geometry_format='wkt'):\n    \"\"\"Coerce specified columns on a cleaned DataFrame using coercion helpers.\n    `dtype_map` is an alternative mapping where keys are 'int','float','datetime','bool','category','geometry'.\n    `geometry_format` controls geometry coercion: 'wkt' (parse WKT strings) or 'geodataframe' (skip, already typed).\"\"\"\n    if not isinstance(df, pd.DataFrame):\n        logger.error('coerce_the_gsheets: input is not a DataFrame')\n        return df\n    return coerce_columns(df, int_cols=int_cols, float_cols=float_cols, datetime_cols=datetime_cols, bool_cols=bool_cols, category_cols=category_cols, geometry_cols=geometry_cols, dtype_map=dtype_map, geometry_format=geometry_format)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoPandas: Working with Shapefiles\n",
    "\n",
    "For spatial data (shapefiles, GeoJSON, etc.), use **geopandas** instead of shapely string parsing. GeoPandas provides:\n",
    "- Direct shapefile loading into GeoDataFrames\n",
    "- Geometry column automatically typed as `geometry`\n",
    "- Spatial operations (spatial joins, intersects, buffer, etc.)\n",
    "- Better integration with tabular data\n",
    "\n",
    "**Loading shapefiles:**\n",
    "```python\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load a shapefile directly (geometry column is already correct type)\n",
    "gdf = gpd.read_file('path/to/shapefile.shp')\n",
    "\n",
    "# Clean tabular columns (geometry is preserved)\n",
    "gdf_clean = clean_the_gsheets(gdf)\n",
    "\n",
    "# Coerce columns, skipping geometry since it's already a GeoSeries\n",
    "gdf_coerced = coerce_the_gsheets(\n",
    "    gdf_clean,\n",
    "    int_cols=['id', 'repl_no'],\n",
    "    float_cols=['area', 'value'],\n",
    "    datetime_cols=['created_at'],\n",
    "    geometry_cols=['geometry'],\n",
    "    geometry_format='geodataframe'  # geometry is already properly typed\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframes(dataframes, normalize_columns):\n",
    "    \"\"\"Normalizes a list of DataFrames by replacing name columns with foreign key IDs.\n",
    "\n",
    "    This function iterates through a list of dataframes and, for each one, iterates\n",
    "    through a dictionary of columns that need to be normalized. It uses the \n",
    "    `replace_name_with_id_df` utility to look up or create the corresponding ID\n",
    "    in the database.\n",
    "\n",
    "    Args:\n",
    "        dataframes (list[pd.DataFrame]): A list of DataFrames to normalize.\n",
    "        normalize_columns (dict): A dictionary mapping column names to SQLModel classes and attributes.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: The list of normalized DataFrames.\n",
    "    \"\"\"\n",
    "    logger.info(f'Starting normalization process for {len(dataframes)} dataframes.')\n",
    "    normalized_dfs = []\n",
    "    try:\n",
    "        with Session(engine) as db:\n",
    "            for i, df in enumerate(dataframes):\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    logger.warning(f'Item {i+1} is not a DataFrame, skipping.')\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f'Processing DataFrame #{i+1} with {len(df)} rows.')\n",
    "                df_normalized = df.copy()\n",
    "\n",
    "                for df_col, (model, model_name_attr) in normalize_columns.items():\n",
    "                    if df_col not in df_normalized.columns:\n",
    "                        logger.warning(f\"Column '{df_col}' not in DataFrame #{i+1}. Skipping normalization for this column.\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Skip normalization if the column is all NaN/None\n",
    "                        if df_normalized[df_col].isnull().all():\n",
    "                            logger.info(f\"Skipping normalization for column '{df_col}' as it contains only null values.\")\n",
    "                            continue\n",
    "                            \n",
    "                        logger.info(f\"Normalizing column '{df_col}' using model '{model.__name__}'.\")\n",
    "                        df_normalized, num_created = replace_name_with_id_df(\n",
    "                            db=db,\n",
    "                            df=df_normalized,\n",
    "                            ref_model=model,\n",
    "                            df_name_column=df_col,\n",
    "                            model_name_attr=model_name_attr,\n",
    "                            id_column_name='id',\n",
    "                            final_column_name=f'{df_col}_id'\n",
    "                        )\n",
    "                        if num_created > 0:\n",
    "                            logger.info(f\"Created {num_created} new records in '{model.__name__}' table.\")\n",
    "                        new_col_name = f'{df_col}_id'\n",
    "                        num_nulls = df_normalized[new_col_name].isnull().sum()\n",
    "                        logger.info(f\"Successfully normalized '{df_col}'. New column '{new_col_name}' contains {num_nulls} null values.\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error normalizing column '{df_col}' in DataFrame #{i+1}: {e}\", exc_info=True)\n",
    "                        continue # Continue to the next column\n",
    "                \n",
    "                normalized_dfs.append(df_normalized)\n",
    "                logger.info(f'Finished processing DataFrame #{i+1}.')\n",
    "            \n",
    "            logger.info('Committing database session.')\n",
    "            db.commit()\n",
    "            logger.info('Database commit successful.')\n",
    "    except Exception as e:\n",
    "        logger.error(f'A critical error occurred during the database session: {e}', exc_info=True)\n",
    "        db.rollback()\n",
    "        logger.info('Database session rolled back.')\n",
    "        \n",
    "    return normalized_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Execution Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Geospatial cleaning example: standardize all lat/lon columns at once\nfrom ca_biositing.pipeline.utils.cleaning_functions import (\n    standardize_latlon,\n    detect_latlon_columns,\n)\n\n# Example DataFrame with mixed lat/lon formats\nsample_geo_df = pd.DataFrame({\n    'site_id': [1, 2, 3, 4, 5],\n    'sampling_lat': [40.7128, 34.0522, 37.7749, '41.8781', None],\n    'sampling_lon': [-74.0060, -118.2437, -122.4194, '-87.6298', -120.0],\n    'prod_location': ['40.5,-74.0', '34.2,-118.5', None, '', '37.5,-122.5'],\n    'name': ['NYC', 'LA', 'SF', 'Chicago', 'Sacramento'],\n})\n\nprint(\"Before standardization:\")\ndf\n\n\n# Auto-detect all lat/lon columns and standardize\nsample_geo_standardized = standardize_latlon(\n    df,\n    auto_detect=True,\n    output_lat='lat',\n    output_lon='lon',\n    coerce_to_float=True\n)\n\nprint(\"\\nAfter standardization:\")\nprint(f\"\\nData types: {sample_geo_standardized[['lat', 'lon']].dtypes.to_dict()}\")\nsample_geo_standardized"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalized_dataframes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
