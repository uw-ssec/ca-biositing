"""getting db back to analysis etl state

Revision ID: 20dedc50864f
Revises: 923d39b6c4da
Create Date: 2026-02-19 13:36:08.337365

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
import sqlmodel

# revision identifiers, used by Alembic.
revision: str = '20dedc50864f'
down_revision: Union[str, Sequence[str], None] = '923d39b6c4da'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    #op.alter_column('billion_ton2023_record', 'production_energy_content',
    #           existing_type=sa.BIGINT(),
    #           type_=sa.Integer(),
    #           existing_nullable=True)
    #op.drop_constraint(op.f('billion_ton2023_record_dataset_id_fkey'), 'billion_ton2023_record', type_='foreignkey')
    #op.drop_column('billion_ton2023_record', 'dataset_id')
    op.add_column('file_object_metadata', sa.Column('uri', sqlmodel.sql.sqltypes.AutoString(), nullable=True))
    op.create_unique_constraint(None, 'file_object_metadata', ['uri'])
    op.drop_constraint(op.f('observation_unique_key'), 'observation', type_='unique')
    op.create_unique_constraint('observation_unique_key', 'observation', ['record_id', 'record_type', 'parameter_id', 'unit_id'])
    #op.drop_column('usda_commodity', 'created_at')
    #op.drop_column('usda_commodity', 'updated_at')
    #op.drop_column('usda_commodity', 'api_name')
    #op.drop_column('usda_survey_record', 'begin_code')
    #op.drop_column('usda_survey_record', 'end_code')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    #op.add_column('usda_survey_record', sa.Column('end_code', sa.INTEGER(), autoincrement=False, nullable=True))
    #op.add_column('usda_survey_record', sa.Column('begin_code', sa.INTEGER(), autoincrement=False, nullable=True))
    #op.add_column('usda_commodity', sa.Column('api_name', sa.VARCHAR(), autoincrement=False, nullable=True))
    #op.add_column('usda_commodity', sa.Column('updated_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True))
    #op.add_column('usda_commodity', sa.Column('created_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True))
    op.drop_constraint(None, 'observation', type_='unique')
    op.create_unique_constraint(op.f('observation_unique_key'), 'observation', ['record_id', 'record_type', 'parameter_id', 'unit_id'], postgresql_nulls_not_distinct=False)
    op.drop_constraint(None, 'file_object_metadata', type_='unique')
    op.drop_column('file_object_metadata', 'uri')
    #op.add_column('billion_ton2023_record', sa.Column('dataset_id', sa.INTEGER(), autoincrement=False, nullable=True))
    #op.create_foreign_key(op.f('billion_ton2023_record_dataset_id_fkey'), 'billion_ton2023_record', 'dataset', ['dataset_id'], ['id'])
    #op.alter_column('billion_ton2023_record', 'production_energy_content',
    #           existing_type=sa.Integer(),
    #           type_=sa.BIGINT(),
    #           existing_nullable=True)
    # ### end Alembic commands ###
