"""Add DeconVessel model and vessel_id to PretreatmentRecord

Revision ID: 311148551786
Revises: 20dedc50864f
Create Date: 2026-02-19 14:23:41.457754

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
import sqlmodel

# revision identifiers, used by Alembic.
revision: str = '311148551786'
down_revision: Union[str, Sequence[str], None] = '20dedc50864f'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('decon_vessel',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('name', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.Column('description', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.Column('uri', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.Column('vessel_uuid', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.Column('serial_number', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.Column('volume_numeric_per_well', sa.Numeric(), nullable=True),
    sa.Column('volume_unit', sqlmodel.sql.sqltypes.AutoString(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('vessel_uuid')
    )
    #op.alter_column('billion_ton2023_record', 'production_energy_content',
    #           existing_type=sa.BIGINT(),
    #           type_=sa.Integer(),
    #           existing_nullable=True)
    #op.drop_constraint(op.f('billion_ton2023_record_dataset_id_fkey'), 'billion_ton2023_record', type_='foreignkey')
    #op.drop_column('billion_ton2023_record', 'dataset_id')
    op.add_column('pretreatment_record', sa.Column('vessel_id', sa.Integer(), nullable=True))
    op.create_foreign_key(None, 'pretreatment_record', 'decon_vessel', ['vessel_id'], ['id'])
    #op.drop_column('usda_commodity', 'api_name')
    #op.drop_column('usda_commodity', 'updated_at')
    #op.drop_column('usda_commodity', 'created_at')
    #op.drop_column('usda_survey_record', 'begin_code')
    #op.drop_column('usda_survey_record', 'end_code')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    #op.add_column('usda_survey_record', sa.Column('end_code', sa.INTEGER(), autoincrement=False, nullable=True))
    #op.add_column('usda_survey_record', sa.Column('begin_code', sa.INTEGER(), autoincrement=False, nullable=True))
    #op.add_column('usda_commodity', sa.Column('created_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True))
    #op.add_column('usda_commodity', sa.Column('updated_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True))
    #op.add_column('usda_commodity', sa.Column('api_name', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.drop_constraint(None, 'pretreatment_record', type_='foreignkey')
    op.drop_column('pretreatment_record', 'vessel_id')
    #op.add_column('billion_ton2023_record', sa.Column('dataset_id', sa.INTEGER(), autoincrement=False, nullable=True))
    #op.create_foreign_key(op.f('billion_ton2023_record_dataset_id_fkey'), 'billion_ton2023_record', 'dataset', ['dataset_id'], ['id'])
    #op.alter_column('billion_ton2023_record', 'production_energy_content',
    #           existing_type=sa.Integer(),
    #           type_=sa.BIGINT(),
    #           existing_nullable=True)
    op.drop_table('decon_vessel')
    # ### end Alembic commands ###
